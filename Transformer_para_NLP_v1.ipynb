{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRANSFORMER para la Traducción de Texto\n",
    "\n",
    "> Basado en:https://www.udemy.com/course/procesamiento-del-lenguaje-natural/learn/lecture/21502260#overview\n",
    "\n",
    "- Author: Juan Gabriel Gomila\n",
    "- Course: \"Procesamiento del Lenguaje Natural Moderno en Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m9JJ7FBw84tG"
   },
   "source": [
    "## Importar las dependencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t5DbIHC-F6Hf"
   },
   "source": [
    "**Paper original**: All you need is Attention https://arxiv.org/pdf/1706.03762.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZbcvtPlp3YWu"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P6o_cpZz3y_-"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    %tensorflow_version 2.x\n",
    "except:\n",
    "    pass\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlearner.nlp import Transformer\n",
    "from mlearner.nlp import Processor_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key savefig.frameon in file C:\\Users\\AUTIS\\Anaconda3\\envs\\Tensorflow 2.0\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle, line 421 ('savefig.frameon : True')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.3.0/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key verbose.level in file C:\\Users\\AUTIS\\Anaconda3\\envs\\Tensorflow 2.0\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle, line 472 ('verbose.level  : silent      # one of silent, helpful, debug, debug-annoying')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.3.0/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key verbose.fileo in file C:\\Users\\AUTIS\\Anaconda3\\envs\\Tensorflow 2.0\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle, line 473 ('verbose.fileo  : sys.stdout  # a log filename, sys.stdout or sys.stderr')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.3.0/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "In C:\\Users\\AUTIS\\Anaconda3\\envs\\Tensorflow 2.0\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\AUTIS\\Anaconda3\\envs\\Tensorflow 2.0\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\AUTIS\\Anaconda3\\envs\\Tensorflow 2.0\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "In C:\\Users\\AUTIS\\Anaconda3\\envs\\Tensorflow 2.0\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\AUTIS\\Anaconda3\\envs\\Tensorflow 2.0\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\AUTIS\\Anaconda3\\envs\\Tensorflow 2.0\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\AUTIS\\Anaconda3\\envs\\Tensorflow 2.0\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\AUTIS\\Anaconda3\\envs\\Tensorflow 2.0\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BQN8jwx48_yU"
   },
   "source": [
    "## Pre Procesado de Datos\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bPlOT-2mlw0r"
   },
   "source": [
    "## Carga de Ficheros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q8Or0sLV5b8t"
   },
   "outputs": [],
   "source": [
    "with open(\"data/europarl-v7.es-en.en\", \n",
    "          mode = \"r\", encoding = \"utf-8\") as f:\n",
    "    europarl_en = f.read()\n",
    "with open(\"data/europarl-v7.es-en.es\", \n",
    "          mode = \"r\", encoding = \"utf-8\") as f:\n",
    "    europarl_es = f.read()\n",
    "with open(\"data/P85-Non-Breaking-Prefix.en\", \n",
    "          mode = \"r\", encoding = \"utf-8\") as f:\n",
    "    non_breaking_prefix_en = f.read()\n",
    "with open(\"data/P85-Non-Breaking-Prefix.en\", \n",
    "          mode = \"r\", encoding = \"utf-8\") as f:\n",
    "    non_breaking_prefix_es = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 58
    },
    "colab_type": "code",
    "id": "TMAFFdpIyNZd",
    "outputId": "eb684c33-6895-4fc9-a27d-420c05a5fab8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Resumption of the session\\nI declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive peri'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "europarl_en[:230]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 58
    },
    "colab_type": "code",
    "id": "BYgCMq6myYIi",
    "outputId": "f76b75fd-25df-43f2-fe4b-f88de485e69d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reanudación del período de sesiones\\nDeclaro reanudado el período de sesiones del Parlamento Europeo, interrumpido el viernes 17 de diciembre pasado, y reitero a Sus Señorías mi deseo de que hayan tenido unas buenas vacaciones'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "europarl_es[:225]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TEFw0D2vP_Dl"
   },
   "source": [
    "## Limpieza de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos funcion de procesado de texto basada en expresiones regulares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Function_clean(text):\n",
    "    \n",
    "    # Eliminamos la @ y su mención\n",
    "    text = re.sub(r\"@[A-Za-z0-9]+\", ' ', text)\n",
    "    # Eliminamos los links de las URLs\n",
    "    text = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se procesan los textos para cada uno de los idiomas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor_en = Processor_data(target_vocab_size=2**13, \n",
    "                              language=\"en\", \n",
    "                              function = Function_clean,\n",
    "                              name=\"processor_en\",\n",
    "                             )\n",
    "processor_es = Processor_data(target_vocab_size=2**13, \n",
    "                              language=\"es\", \n",
    "                              function = Function_clean,\n",
    "                              name=\"processor_es\"\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(\"data/corpus_en.csv\"):\n",
    "    corpus_en = europarl_en\n",
    "    corpus_en = processor_en.clean(corpus_en)\n",
    "    pd.DataFrame(corpus_en).to_csv(\"data/corpus_en.csv\", index=False)\n",
    "\n",
    "if not os.path.isfile(\"data/corpus_es.csv\"):\n",
    "    corpus_es = europarl_es\n",
    "    corpus_es = processor_es.clean(corpus_es)\n",
    "    pd.DataFrame(corpus_es).to_csv(\"data/corpus_es.csv\", index=False)\n",
    "\n",
    "corpus_en = pd.read_csv(\"data/corpus_en.csv\")    \n",
    "corpus_es = pd.read_csv(\"data/corpus_es.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploramos los textos para cada idioma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Resumption of the session</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I declare resumed the session of the European ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0                          Resumption of the session\n",
       "1  I declare resumed the session of the European ..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_en[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Reanudación del período de sesiones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Declaro reanudado el período de sesiones del P...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0                Reanudación del período de sesiones\n",
       "1  Declaro reanudado el período de sesiones del P..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_es[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1965735, 1965735)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_en), len(corpus_es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s-Y9v8-Tozl2"
   },
   "source": [
    "## Tokenizar el Texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizado del texto sin aplicar limpieza (aplicada en el apartado anterior) y sin padding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('data/processor_en.joblib'):\n",
    "    tokens_en = processor_en.process_text(corpus_en, \n",
    "                                             isclean=True, \n",
    "                                             padding=False)\n",
    "    dump(processor_en, 'data/processor_en.joblib')\n",
    "else:\n",
    "    processor_en = load('data/processor_en.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('data/processor_es.joblib'):\n",
    "    tokens_es = processor_es.process_text(corpus_es, \n",
    "                                             isclean=True, \n",
    "                                             padding=False)\n",
    "    dump(processor_es, 'data/processor_es.joblib')\n",
    "else:\n",
    "    processor_es = load('data/processor_es.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tamaño de Vocabulario para los dos idiomas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ftIbPzIwCtwL"
   },
   "outputs": [],
   "source": [
    "if not os.path.isfile(\"data/inputs.csv\") and not os.path.isfile(\"data/outputs.csv\"):\n",
    "    VOCAB_SIZE_EN = processor_en.tokenizer.vocab_size + 2\n",
    "    VOCAB_SIZE_ES = processor_es.tokenizer.vocab_size + 2\n",
    "\n",
    "    print(VOCAB_SIZE_EN, VOCAB_SIZE_ES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sustituimos los valores NaN con valores vacios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(\"data/inputs.csv\") and not os.path.isfile(\"data/outputs.csv\"):\n",
    "    corpus_es = corpus_es.fillna(\" \")\n",
    "    corpus_en = corpus_en.fillna(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparación de las frases como inputs/outputs del Modelo:\n",
    "\n",
    "> _**[ \\INICIO ]**_ + frase + _**[ \\FIN ]**_\n",
    "\n",
    "- **[ \\INICIO ]**: Carácter que determina el inicio de frase.\n",
    "- **[ \\FIN ]**: Carácter que determina el final de frase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oPFe2YJDC9jw"
   },
   "outputs": [],
   "source": [
    "if not os.path.isfile(\"data/inputs.csv\") and not os.path.isfile(\"data/outputs.csv\"):\n",
    "    inputs = [[VOCAB_SIZE_EN-2] + \\\n",
    "              processor_en.tokenizer.encode(sentence[0]) + [VOCAB_SIZE_EN-1] \\\n",
    "                for sentence in corpus_en.values]\n",
    "\n",
    "    outputs = [[VOCAB_SIZE_ES-2] + \\\n",
    "               processor_es.tokenizer.encode(sentence[0]) + [VOCAB_SIZE_ES-1] \n",
    "                for sentence in corpus_es.values ]\n",
    "    len(inputs), len(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bG6AlcFMpC5C"
   },
   "source": [
    "## Eliminamos las frases demasiado largas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F6CD6PLGyQWy"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 20\n",
    "\n",
    "if not os.path.isfile(\"data/inputs.csv\") and not os.path.isfile(\"data/outputs.csv\"):\n",
    "    idx_to_remove = [count for count, sent in enumerate(inputs)\n",
    "                     if len(sent) > MAX_LENGTH]\n",
    "    if len(idx_to_remove) > 0:\n",
    "        for idx in reversed(idx_to_remove):\n",
    "            del inputs[idx]\n",
    "            del outputs[idx]\n",
    "\n",
    "    idx_to_remove = [count for count, sent in enumerate(outputs)\n",
    "                     if len(sent) > MAX_LENGTH]\n",
    "    if len(idx_to_remove) > 0:\n",
    "        for idx in reversed(idx_to_remove):\n",
    "            del inputs[idx]\n",
    "            del outputs[idx]\n",
    "\n",
    "    pd.DataFrame(inputs).to_csv(\"data/inputs.csv\", index=False)\n",
    "    pd.DataFrame(outputs).to_csv(\"data/outputs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ypm8h5aZQTZ1"
   },
   "source": [
    "## Creamos las entradas y las salidas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9FP0WPsdM8hl"
   },
   "source": [
    "A medida que entrenamos con bloques, necesitaremos que cada entrada tenga la misma longitud. Rellenamos con el token apropiado, y nos aseguraremos de que este token de relleno no interfiera con nuestro entrenamiento más adelante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(411131, 411131)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = pd.read_csv(\"data/inputs.csv\").fillna(0).astype(int)   \n",
    "outputs = pd.read_csv(\"data/outputs.csv\").fillna(0).astype(int)\n",
    "\n",
    "len(inputs), len(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nvDfLDWUONlE"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 20\n",
    "\n",
    "VOCAB_SIZE_EN = 8198\n",
    "VOCAB_SIZE_ES = 8225 \n",
    "\n",
    "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs.values,\n",
    "                                                       value=0,\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=MAX_LENGTH)\n",
    "outputs = tf.keras.preprocessing.sequence.pad_sequences(outputs.values,\n",
    "                                                        value=0,\n",
    "                                                        padding='post',\n",
    "                                                        maxlen=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea el daset generador para servir los inputs/outputs procesados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wFxMp3TOIYff"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-c-LRThUPrso"
   },
   "source": [
    "## Modelo Transformer - Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qiOdqQ5qPs8z"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Hiper Parámetros\n",
    "D_MODEL = 128 # 512\n",
    "NB_LAYERS = 4 # 6\n",
    "FFN_UNITS = 512 # 2048\n",
    "NB_PROJ = 8 # 8\n",
    "DROPOUT_RATE = 0.1 # 0.1\n",
    "\n",
    "model_Transformer = Transformer(vocab_size_enc=VOCAB_SIZE_EN,\n",
    "                          vocab_size_dec=VOCAB_SIZE_ES,\n",
    "                          d_model=D_MODEL,\n",
    "                          nb_layers=NB_LAYERS,\n",
    "                          FFN_units=FFN_UNITS,\n",
    "                          nb_proj=NB_PROJ,\n",
    "                          dropout_rate=DROPOUT_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bucle de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlearner.nlp import Transformer_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The last checkpoint has been restored\n",
      "Inicio del epoch 1\n",
      "Epoch 1 Lote 0 Pérdida 1.5254 Precisión 0.4120\n",
      "Epoch 1 Lote 50 Pérdida 1.2794 Precisión 0.4312\n",
      "Epoch 1 Lote 100 Pérdida 1.2828 Precisión 0.4283\n",
      "Epoch 1 Lote 150 Pérdida 1.2888 Precisión 0.4290\n",
      "Epoch 1 Lote 200 Pérdida 1.2800 Precisión 0.4284\n",
      "Epoch 1 Lote 250 Pérdida 1.2772 Precisión 0.4293\n",
      "Epoch 1 Lote 300 Pérdida 1.2802 Precisión 0.4306\n",
      "Epoch 1 Lote 350 Pérdida 1.2769 Precisión 0.4307\n",
      "Epoch 1 Lote 400 Pérdida 1.2755 Precisión 0.4304\n",
      "Epoch 1 Lote 450 Pérdida 1.2708 Precisión 0.4305\n",
      "Epoch 1 Lote 500 Pérdida 1.2662 Precisión 0.4306\n",
      "Epoch 1 Lote 550 Pérdida 1.2648 Precisión 0.4307\n",
      "Epoch 1 Lote 600 Pérdida 1.2632 Precisión 0.4307\n",
      "Epoch 1 Lote 650 Pérdida 1.2624 Precisión 0.4299\n",
      "Epoch 1 Lote 700 Pérdida 1.2614 Precisión 0.4303\n",
      "Epoch 1 Lote 750 Pérdida 1.2577 Precisión 0.4314\n",
      "Epoch 1 Lote 800 Pérdida 1.2538 Precisión 0.4324\n",
      "Epoch 1 Lote 850 Pérdida 1.2489 Precisión 0.4335\n",
      "Epoch 1 Lote 900 Pérdida 1.2450 Precisión 0.4343\n",
      "Epoch 1 Lote 950 Pérdida 1.2380 Precisión 0.4354\n",
      "Epoch 1 Lote 1000 Pérdida 1.2323 Precisión 0.4365\n",
      "Epoch 1 Lote 1050 Pérdida 1.2264 Precisión 0.4372\n",
      "Epoch 1 Lote 1100 Pérdida 1.2207 Precisión 0.4380\n",
      "Epoch 1 Lote 1150 Pérdida 1.2137 Precisión 0.4389\n",
      "Epoch 1 Lote 1200 Pérdida 1.2080 Precisión 0.4398\n",
      "Epoch 1 Lote 1250 Pérdida 1.2020 Precisión 0.4408\n",
      "Epoch 1 Lote 1300 Pérdida 1.1955 Precisión 0.4415\n",
      "Epoch 1 Lote 1350 Pérdida 1.1900 Precisión 0.4424\n",
      "Epoch 1 Lote 1400 Pérdida 1.1843 Precisión 0.4433\n",
      "Epoch 1 Lote 1450 Pérdida 1.1792 Precisión 0.4442\n",
      "Epoch 1 Lote 1500 Pérdida 1.1741 Precisión 0.4449\n",
      "Epoch 1 Lote 1550 Pérdida 1.1695 Precisión 0.4457\n",
      "Epoch 1 Lote 1600 Pérdida 1.1649 Precisión 0.4463\n",
      "Epoch 1 Lote 1650 Pérdida 1.1599 Precisión 0.4470\n",
      "Epoch 1 Lote 1700 Pérdida 1.1549 Precisión 0.4478\n",
      "Epoch 1 Lote 1750 Pérdida 1.1498 Precisión 0.4484\n",
      "Epoch 1 Lote 1800 Pérdida 1.1446 Precisión 0.4489\n",
      "Epoch 1 Lote 1850 Pérdida 1.1400 Precisión 0.4494\n",
      "Epoch 1 Lote 1900 Pérdida 1.1358 Precisión 0.4498\n",
      "Epoch 1 Lote 1950 Pérdida 1.1318 Precisión 0.4502\n",
      "Epoch 1 Lote 2000 Pérdida 1.1280 Precisión 0.4506\n",
      "Epoch 1 Lote 2050 Pérdida 1.1236 Precisión 0.4509\n",
      "Epoch 1 Lote 2100 Pérdida 1.1199 Precisión 0.4513\n",
      "Epoch 1 Lote 2150 Pérdida 1.1165 Precisión 0.4517\n",
      "Epoch 1 Lote 2200 Pérdida 1.1131 Precisión 0.4521\n",
      "Epoch 1 Lote 2250 Pérdida 1.1094 Precisión 0.4526\n",
      "Epoch 1 Lote 2300 Pérdida 1.1061 Precisión 0.4530\n",
      "Epoch 1 Lote 2350 Pérdida 1.1028 Precisión 0.4535\n",
      "Epoch 1 Lote 2400 Pérdida 1.0990 Precisión 0.4538\n",
      "Epoch 1 Lote 2450 Pérdida 1.0954 Precisión 0.4541\n",
      "Epoch 1 Lote 2500 Pérdida 1.0923 Precisión 0.4545\n",
      "Epoch 1 Lote 2550 Pérdida 1.0885 Precisión 0.4550\n",
      "Epoch 1 Lote 2600 Pérdida 1.0849 Precisión 0.4554\n",
      "Epoch 1 Lote 2650 Pérdida 1.0816 Precisión 0.4557\n",
      "Epoch 1 Lote 2700 Pérdida 1.0792 Precisión 0.4561\n",
      "Epoch 1 Lote 2750 Pérdida 1.0762 Precisión 0.4564\n",
      "Epoch 1 Lote 2800 Pérdida 1.0737 Precisión 0.4568\n",
      "Epoch 1 Lote 2850 Pérdida 1.0713 Precisión 0.4571\n",
      "Epoch 1 Lote 2900 Pérdida 1.0691 Precisión 0.4575\n",
      "Epoch 1 Lote 2950 Pérdida 1.0671 Precisión 0.4578\n",
      "Epoch 1 Lote 3000 Pérdida 1.0650 Precisión 0.4581\n",
      "Epoch 1 Lote 3050 Pérdida 1.0632 Precisión 0.4585\n",
      "Epoch 1 Lote 3100 Pérdida 1.0613 Precisión 0.4588\n",
      "Epoch 1 Lote 3150 Pérdida 1.0596 Precisión 0.4592\n",
      "Epoch 1 Lote 3200 Pérdida 1.0582 Precisión 0.4596\n",
      "Epoch 1 Lote 3250 Pérdida 1.0563 Precisión 0.4600\n",
      "Epoch 1 Lote 3300 Pérdida 1.0545 Precisión 0.4604\n",
      "Epoch 1 Lote 3350 Pérdida 1.0529 Precisión 0.4608\n",
      "Epoch 1 Lote 3400 Pérdida 1.0515 Precisión 0.4611\n",
      "Epoch 1 Lote 3450 Pérdida 1.0500 Precisión 0.4615\n",
      "Epoch 1 Lote 3500 Pérdida 1.0488 Precisión 0.4619\n",
      "Epoch 1 Lote 3550 Pérdida 1.0474 Precisión 0.4622\n",
      "Epoch 1 Lote 3600 Pérdida 1.0459 Precisión 0.4626\n",
      "Epoch 1 Lote 3650 Pérdida 1.0446 Precisión 0.4630\n",
      "Epoch 1 Lote 3700 Pérdida 1.0430 Precisión 0.4633\n",
      "Epoch 1 Lote 3750 Pérdida 1.0414 Precisión 0.4637\n",
      "Epoch 1 Lote 3800 Pérdida 1.0401 Precisión 0.4640\n",
      "Epoch 1 Lote 3850 Pérdida 1.0387 Precisión 0.4643\n",
      "Epoch 1 Lote 3900 Pérdida 1.0370 Precisión 0.4647\n",
      "Epoch 1 Lote 3950 Pérdida 1.0357 Precisión 0.4650\n",
      "Epoch 1 Lote 4000 Pérdida 1.0345 Precisión 0.4654\n",
      "Epoch 1 Lote 4050 Pérdida 1.0329 Precisión 0.4658\n",
      "Epoch 1 Lote 4100 Pérdida 1.0313 Precisión 0.4661\n",
      "Epoch 1 Lote 4150 Pérdida 1.0295 Precisión 0.4665\n",
      "Epoch 1 Lote 4200 Pérdida 1.0277 Precisión 0.4669\n",
      "Epoch 1 Lote 4250 Pérdida 1.0259 Precisión 0.4673\n",
      "Epoch 1 Lote 4300 Pérdida 1.0242 Precisión 0.4678\n",
      "Epoch 1 Lote 4350 Pérdida 1.0229 Precisión 0.4683\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "\n",
    "Transformer_train(model_Transformer,\n",
    "                  dataset,\n",
    "                  d_model=D_MODEL,\n",
    "                  train=TRAIN,\n",
    "                  epochs=EPOCHS,\n",
    "                  checkpoint_path=\"ckpt/\",\n",
    "                  max_to_keep=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nmzyRwDrRGdq"
   },
   "source": [
    "## Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cNHwJJrz3lPB"
   },
   "outputs": [],
   "source": [
    "def evaluate(inp_sentence):\n",
    "    inp_sentence = \\\n",
    "        [VOCAB_SIZE_EN-2] + processor_en.tokenizer.encode(inp_sentence) + [VOCAB_SIZE_EN-1]\n",
    "    enc_input = tf.expand_dims(inp_sentence, axis=0)\n",
    "    \n",
    "    output = tf.expand_dims([VOCAB_SIZE_ES-2], axis=0)\n",
    "    \n",
    "    for _ in range(MAX_LENGTH):\n",
    "        predictions = model_Transformer(enc_input, output, False) #(1, seq_length, VOCAB_SIZE_ES)\n",
    "        \n",
    "        prediction = predictions[:, -1:, :]\n",
    "        \n",
    "        predicted_id = tf.cast(tf.argmax(prediction, axis=-1), tf.int32)\n",
    "        \n",
    "        if predicted_id == VOCAB_SIZE_ES-1:\n",
    "            return tf.squeeze(output, axis=0)\n",
    "        \n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "        \n",
    "    return tf.squeeze(output, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s6VeFKrE6Kdx"
   },
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    output = evaluate(sentence).numpy()\n",
    "    \n",
    "    predicted_sentence = processor_es.tokenizer.decode(\n",
    "        [i for i in output if i < VOCAB_SIZE_ES-2]\n",
    "    )\n",
    "    \n",
    "    print(\"Entrada: {}\".format(sentence))\n",
    "    print(\"Traducción predicha: {}\".format(predicted_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(\"I have got a house\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "BupFjJlgDvCA",
    "outputId": "54e7aa51-1cc2-42a5-ede2-304d1a230d08"
   },
   "outputs": [],
   "source": [
    "translate(\"This is a problem we have to solve.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "ZdoWKbCP7Czs",
    "outputId": "dbe5425f-1c91-45a7-87ab-4ed0dbf13d74"
   },
   "outputs": [],
   "source": [
    "translate(\"This is a really powerful tool!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "UGjBEb5WFMGt",
    "outputId": "a7892348-eaf4-4445-d816-3c587275f436"
   },
   "outputs": [],
   "source": [
    "translate(\"This is an interesting course about Natural Language Processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L1v9g4T0FcO3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Transformer para NLP.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
