{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRANSFORMER para la Traducción de Texto\n",
    "\n",
    "> Basado en:https://www.udemy.com/course/procesamiento-del-lenguaje-natural/learn/lecture/21502260#overview\n",
    "\n",
    "- Author: Juan Gabriel Gomila\n",
    "- Course: \"Procesamiento del Lenguaje Natural Moderno en Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m9JJ7FBw84tG"
   },
   "source": [
    "## Importar las dependencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t5DbIHC-F6Hf"
   },
   "source": [
    "**Paper original**: All you need is Attention https://arxiv.org/pdf/1706.03762.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZbcvtPlp3YWu"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P6o_cpZz3y_-"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    %tensorflow_version 2.x\n",
    "except:\n",
    "    pass\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlearner.nlp import Transformer\n",
    "from mlearner.nlp import Processor_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BQN8jwx48_yU"
   },
   "source": [
    "## Pre Procesado de Datos\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bPlOT-2mlw0r"
   },
   "source": [
    "## Carga de Ficheros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q8Or0sLV5b8t"
   },
   "outputs": [],
   "source": [
    "with open(\"data/europarl-v7.es-en.en\", \n",
    "          mode = \"r\", encoding = \"utf-8\") as f:\n",
    "    europarl_en = f.read()\n",
    "with open(\"data/europarl-v7.es-en.es\", \n",
    "          mode = \"r\", encoding = \"utf-8\") as f:\n",
    "    europarl_es = f.read()\n",
    "with open(\"data/P85-Non-Breaking-Prefix.en\", \n",
    "          mode = \"r\", encoding = \"utf-8\") as f:\n",
    "    non_breaking_prefix_en = f.read()\n",
    "with open(\"data/P85-Non-Breaking-Prefix.en\", \n",
    "          mode = \"r\", encoding = \"utf-8\") as f:\n",
    "    non_breaking_prefix_es = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 58
    },
    "colab_type": "code",
    "id": "TMAFFdpIyNZd",
    "outputId": "eb684c33-6895-4fc9-a27d-420c05a5fab8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Resumption of the session\\nI declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive peri'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "europarl_en[:230]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 58
    },
    "colab_type": "code",
    "id": "BYgCMq6myYIi",
    "outputId": "f76b75fd-25df-43f2-fe4b-f88de485e69d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reanudación del período de sesiones\\nDeclaro reanudado el período de sesiones del Parlamento Europeo, interrumpido el viernes 17 de diciembre pasado, y reitero a Sus Señorías mi deseo de que hayan tenido unas buenas vacaciones'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "europarl_es[:225]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TEFw0D2vP_Dl"
   },
   "source": [
    "## Limpieza de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos funcion de procesado de texto basada en expresiones regulares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Function_clean(text):\n",
    "    \n",
    "    # Eliminamos la @ y su mención\n",
    "    text = re.sub(r\"@[A-Za-z0-9]+\", ' ', text)\n",
    "    # Eliminamos los links de las URLs\n",
    "    text = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se procesan los textos para cada uno de los idiomas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor_en = Processor_data(target_vocab_size=2**13, \n",
    "                              language=\"en\", \n",
    "                              function = Function_clean,\n",
    "                              name=\"processor_en\",\n",
    "                             )\n",
    "processor_es = Processor_data(target_vocab_size=2**13, \n",
    "                              language=\"es\", \n",
    "                              function = Function_clean,\n",
    "                              name=\"processor_es\"\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(\"data/corpus_en.csv\"):\n",
    "    corpus_en = europarl_en\n",
    "    corpus_en = processor_en.clean(corpus_en)\n",
    "    pd.DataFrame(corpus_en).to_csv(\"data/corpus_en.csv\", index=False)\n",
    "\n",
    "if not os.path.isfile(\"data/corpus_es.csv\"):\n",
    "    corpus_es = europarl_es\n",
    "    corpus_es = processor_es.clean(corpus_es)\n",
    "    pd.DataFrame(corpus_es).to_csv(\"data/corpus_es.csv\", index=False)\n",
    "\n",
    "corpus_en = pd.read_csv(\"data/corpus_en.csv\")    \n",
    "corpus_es = pd.read_csv(\"data/corpus_es.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploramos los textos para cada idioma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Resumption of the session</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I declare resumed the session of the European ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0                          Resumption of the session\n",
       "1  I declare resumed the session of the European ..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_en[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Reanudación del período de sesiones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Declaro reanudado el período de sesiones del P...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0                Reanudación del período de sesiones\n",
       "1  Declaro reanudado el período de sesiones del P..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_es[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1965735, 1965735)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_en), len(corpus_es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s-Y9v8-Tozl2"
   },
   "source": [
    "## Tokenizar el Texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizado del texto sin aplicar limpieza (aplicada en el apartado anterior) y sin padding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('data/processor_en.joblib'):\n",
    "    tokens_en = processor_en.process_text(corpus_en, \n",
    "                                             isclean=True, \n",
    "                                             padding=False)\n",
    "    dump(processor_en, 'data/processor_en.joblib')\n",
    "else:\n",
    "    processor_en = load('data/processor_en.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('data/processor_es.joblib'):\n",
    "    tokens_es = processor_es.process_text(corpus_es, \n",
    "                                             isclean=True, \n",
    "                                             padding=False)\n",
    "    dump(processor_es, 'data/processor_es.joblib')\n",
    "else:\n",
    "    processor_es = load('data/processor_es.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tamaño de Vocabulario para los dos idiomas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ftIbPzIwCtwL"
   },
   "outputs": [],
   "source": [
    "if not os.path.isfile(\"data/inputs.csv\") and not os.path.isfile(\"data/outputs.csv\"):\n",
    "    VOCAB_SIZE_EN = processor_en.tokenizer.vocab_size + 2\n",
    "    VOCAB_SIZE_ES = processor_es.tokenizer.vocab_size + 2\n",
    "\n",
    "    print(VOCAB_SIZE_EN, VOCAB_SIZE_ES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sustituimos los valores NaN con valores vacios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(\"data/inputs.csv\") and not os.path.isfile(\"data/outputs.csv\"):\n",
    "    corpus_es = corpus_es.fillna(\" \")\n",
    "    corpus_en = corpus_en.fillna(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparación de las frases como inputs/outputs del Modelo:\n",
    "\n",
    "> _**[ \\INICIO ]**_ + frase + _**[ \\FIN ]**_\n",
    "\n",
    "- **[ \\INICIO ]**: Carácter que determina el inicio de frase.\n",
    "- **[ \\FIN ]**: Carácter que determina el final de frase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oPFe2YJDC9jw"
   },
   "outputs": [],
   "source": [
    "if not os.path.isfile(\"data/inputs.csv\") and not os.path.isfile(\"data/outputs.csv\"):\n",
    "    inputs = [[VOCAB_SIZE_EN-2] + \\\n",
    "              processor_en.tokenizer.encode(sentence[0]) + [VOCAB_SIZE_EN-1] \\\n",
    "                for sentence in corpus_en.values]\n",
    "\n",
    "    outputs = [[VOCAB_SIZE_ES-2] + \\\n",
    "               processor_es.tokenizer.encode(sentence[0]) + [VOCAB_SIZE_ES-1] \n",
    "                for sentence in corpus_es.values ]\n",
    "    len(inputs), len(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bG6AlcFMpC5C"
   },
   "source": [
    "## Eliminamos las frases demasiado largas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F6CD6PLGyQWy"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 20\n",
    "\n",
    "if not os.path.isfile(\"data/inputs.csv\") and not os.path.isfile(\"data/outputs.csv\"):\n",
    "    idx_to_remove = [count for count, sent in enumerate(inputs)\n",
    "                     if len(sent) > MAX_LENGTH]\n",
    "    if len(idx_to_remove) > 0:\n",
    "        for idx in reversed(idx_to_remove):\n",
    "            del inputs[idx]\n",
    "            del outputs[idx]\n",
    "\n",
    "    idx_to_remove = [count for count, sent in enumerate(outputs)\n",
    "                     if len(sent) > MAX_LENGTH]\n",
    "    if len(idx_to_remove) > 0:\n",
    "        for idx in reversed(idx_to_remove):\n",
    "            del inputs[idx]\n",
    "            del outputs[idx]\n",
    "\n",
    "    pd.DataFrame(inputs).to_csv(\"data/inputs.csv\", index=False)\n",
    "    pd.DataFrame(outputs).to_csv(\"data/outputs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ypm8h5aZQTZ1"
   },
   "source": [
    "## Creamos las entradas y las salidas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9FP0WPsdM8hl"
   },
   "source": [
    "A medida que entrenamos con bloques, necesitaremos que cada entrada tenga la misma longitud. Rellenamos con el token apropiado, y nos aseguraremos de que este token de relleno no interfiera con nuestro entrenamiento más adelante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(411131, 411131)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = pd.read_csv(\"data/inputs.csv\").fillna(0).astype(int)   \n",
    "outputs = pd.read_csv(\"data/outputs.csv\").fillna(0).astype(int)\n",
    "\n",
    "len(inputs), len(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nvDfLDWUONlE"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 20\n",
    "\n",
    "VOCAB_SIZE_EN = 8198\n",
    "VOCAB_SIZE_ES = 8225 \n",
    "\n",
    "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs.values,\n",
    "                                                       value=0,\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=MAX_LENGTH)\n",
    "outputs = tf.keras.preprocessing.sequence.pad_sequences(outputs.values,\n",
    "                                                        value=0,\n",
    "                                                        padding='post',\n",
    "                                                        maxlen=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea el daset generador para servir los inputs/outputs procesados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wFxMp3TOIYff"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-c-LRThUPrso"
   },
   "source": [
    "## Modelo Transformer - Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qiOdqQ5qPs8z"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Hiper Parámetros\n",
    "D_MODEL = 128 # 512\n",
    "NB_LAYERS = 4 # 6\n",
    "FFN_UNITS = 512 # 2048\n",
    "NB_PROJ = 8 # 8\n",
    "DROPOUT_RATE = 0.1 # 0.1\n",
    "\n",
    "model_Transformer = Transformer(vocab_size_enc=VOCAB_SIZE_EN,\n",
    "                          vocab_size_dec=VOCAB_SIZE_ES,\n",
    "                          d_model=D_MODEL,\n",
    "                          nb_layers=NB_LAYERS,\n",
    "                          FFN_units=FFN_UNITS,\n",
    "                          nb_proj=NB_PROJ,\n",
    "                          dropout_rate=DROPOUT_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bucle de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlearner.nlp import Transformer_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The last checkpoint has been restored\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 2\n",
    "\n",
    "Transformer_train(model_Transformer,\n",
    "                  dataset,\n",
    "                  d_model=D_MODEL,\n",
    "                  train=TRAIN,\n",
    "                  epochs=EPOCHS,\n",
    "                  checkpoint_path=\"ckpt/\",\n",
    "                  max_to_keep=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nmzyRwDrRGdq"
   },
   "source": [
    "## Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cNHwJJrz3lPB"
   },
   "outputs": [],
   "source": [
    "def evaluate(inp_sentence):\n",
    "    inp_sentence = \\\n",
    "        [VOCAB_SIZE_EN-2] + processor_en.tokenizer.encode(inp_sentence) + [VOCAB_SIZE_EN-1]\n",
    "    enc_input = tf.expand_dims(inp_sentence, axis=0)\n",
    "    \n",
    "    output = tf.expand_dims([VOCAB_SIZE_ES-2], axis=0)\n",
    "    \n",
    "    for _ in range(MAX_LENGTH):\n",
    "        predictions = model_Transformer(enc_input, output, False) #(1, seq_length, VOCAB_SIZE_ES)\n",
    "        \n",
    "        prediction = predictions[:, -1:, :]\n",
    "        \n",
    "        predicted_id = tf.cast(tf.argmax(prediction, axis=-1), tf.int32)\n",
    "        \n",
    "        if predicted_id == VOCAB_SIZE_ES-1:\n",
    "            return tf.squeeze(output, axis=0)\n",
    "        \n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "        \n",
    "    return tf.squeeze(output, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s6VeFKrE6Kdx"
   },
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    output = evaluate(sentence).numpy()\n",
    "    \n",
    "    predicted_sentence = processor_es.tokenizer.decode(\n",
    "        [i for i in output if i < VOCAB_SIZE_ES-2]\n",
    "    )\n",
    "    \n",
    "    print(\"Entrada: {}\".format(sentence))\n",
    "    print(\"Traducción predicha: {}\".format(predicted_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrada: I have got a house\n",
      "Traducción predicha: Yo tengo una casa.\n"
     ]
    }
   ],
   "source": [
    "translate(\"I have got a house\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "BupFjJlgDvCA",
    "outputId": "54e7aa51-1cc2-42a5-ede2-304d1a230d08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrada: This is a problem we have to solve.\n",
      "Traducción predicha: Este es un problema que tenemos que resolver.\n"
     ]
    }
   ],
   "source": [
    "translate(\"This is a problem we have to solve.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "ZdoWKbCP7Czs",
    "outputId": "dbe5425f-1c91-45a7-87ab-4ed0dbf13d74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrada: This is a really powerful tool!\n",
      "Traducción predicha: Esto es realmente un simple edificio.\n"
     ]
    }
   ],
   "source": [
    "translate(\"This is a really powerful tool!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "UGjBEb5WFMGt",
    "outputId": "a7892348-eaf4-4445-d816-3c587275f436"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrada: This is an interesting course about Natural Language Processing\n",
      "Traducción predicha: Es un error interesante sobre la categoría transfronteriza de la carne de la Comunidad.\n"
     ]
    }
   ],
   "source": [
    "translate(\"This is an interesting course about Natural Language Processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L1v9g4T0FcO3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Transformer para NLP.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
