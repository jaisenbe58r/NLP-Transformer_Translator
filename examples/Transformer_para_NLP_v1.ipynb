{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRANSFORMER para la Traducción de Texto\n",
    "\n",
    "> Basado en:https://www.udemy.com/course/procesamiento-del-lenguaje-natural/learn/lecture/21502260#overview\n",
    "\n",
    "- Author: Juan Gabriel Gomila\n",
    "- Course: \"Procesamiento del Lenguaje Natural Moderno en Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m9JJ7FBw84tG"
   },
   "source": [
    "## Importar las dependencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t5DbIHC-F6Hf"
   },
   "source": [
    "**Paper original**: All you need is Attention https://arxiv.org/pdf/1706.03762.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZbcvtPlp3YWu"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P6o_cpZz3y_-"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    %tensorflow_version 2.x\n",
    "except:\n",
    "    pass\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlearner.nlp import Transformer\n",
    "from mlearner.nlp import Processor_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key savefig.frameon in file C:\\Users\\AUTIS\\Anaconda3\\envs\\Tensorflow 2.0\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle, line 421 ('savefig.frameon : True')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.3.0/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key verbose.level in file C:\\Users\\AUTIS\\Anaconda3\\envs\\Tensorflow 2.0\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle, line 472 ('verbose.level  : silent      # one of silent, helpful, debug, debug-annoying')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.3.0/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key verbose.fileo in file C:\\Users\\AUTIS\\Anaconda3\\envs\\Tensorflow 2.0\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle, line 473 ('verbose.fileo  : sys.stdout  # a log filename, sys.stdout or sys.stderr')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.3.0/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "In C:\\Users\\AUTIS\\Anaconda3\\envs\\Tensorflow 2.0\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\AUTIS\\Anaconda3\\envs\\Tensorflow 2.0\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\AUTIS\\Anaconda3\\envs\\Tensorflow 2.0\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "In C:\\Users\\AUTIS\\Anaconda3\\envs\\Tensorflow 2.0\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\AUTIS\\Anaconda3\\envs\\Tensorflow 2.0\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\AUTIS\\Anaconda3\\envs\\Tensorflow 2.0\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\AUTIS\\Anaconda3\\envs\\Tensorflow 2.0\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\AUTIS\\Anaconda3\\envs\\Tensorflow 2.0\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BQN8jwx48_yU"
   },
   "source": [
    "## Pre Procesado de Datos\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bPlOT-2mlw0r"
   },
   "source": [
    "## Carga de Ficheros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q8Or0sLV5b8t"
   },
   "outputs": [],
   "source": [
    "with open(\"data/europarl-v7.es-en.en\", \n",
    "          mode = \"r\", encoding = \"utf-8\") as f:\n",
    "    europarl_en = f.read()\n",
    "with open(\"data/europarl-v7.es-en.es\", \n",
    "          mode = \"r\", encoding = \"utf-8\") as f:\n",
    "    europarl_es = f.read()\n",
    "with open(\"data/P85-Non-Breaking-Prefix.en\", \n",
    "          mode = \"r\", encoding = \"utf-8\") as f:\n",
    "    non_breaking_prefix_en = f.read()\n",
    "with open(\"data/P85-Non-Breaking-Prefix.en\", \n",
    "          mode = \"r\", encoding = \"utf-8\") as f:\n",
    "    non_breaking_prefix_es = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 58
    },
    "colab_type": "code",
    "id": "TMAFFdpIyNZd",
    "outputId": "eb684c33-6895-4fc9-a27d-420c05a5fab8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Resumption of the session\\nI declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive peri'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "europarl_en[:230]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 58
    },
    "colab_type": "code",
    "id": "BYgCMq6myYIi",
    "outputId": "f76b75fd-25df-43f2-fe4b-f88de485e69d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reanudación del período de sesiones\\nDeclaro reanudado el período de sesiones del Parlamento Europeo, interrumpido el viernes 17 de diciembre pasado, y reitero a Sus Señorías mi deseo de que hayan tenido unas buenas vacaciones'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "europarl_es[:225]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TEFw0D2vP_Dl"
   },
   "source": [
    "## Limpieza de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos funcion de procesado de texto basada en expresiones regulares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Function_clean(text):\n",
    "    \n",
    "    # Eliminamos la @ y su mención\n",
    "    text = re.sub(r\"@[A-Za-z0-9]+\", ' ', text)\n",
    "    # Eliminamos los links de las URLs\n",
    "    text = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se procesan los textos para cada uno de los idiomas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor_en = Processor_data(target_vocab_size=2**13, \n",
    "                              language=\"en\", \n",
    "                              function = Function_clean,\n",
    "                              name=\"processor_en\",\n",
    "                             )\n",
    "processor_es = Processor_data(target_vocab_size=2**13, \n",
    "                              language=\"es\", \n",
    "                              function = Function_clean,\n",
    "                              name=\"processor_es\"\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(\"data/corpus_en.csv\"):\n",
    "    corpus_en = europarl_en\n",
    "    corpus_en = processor_en.clean(corpus_en)\n",
    "    pd.DataFrame(corpus_en).to_csv(\"data/corpus_en.csv\", index=False)\n",
    "\n",
    "if not os.path.isfile(\"data/corpus_es.csv\"):\n",
    "    corpus_es = europarl_es\n",
    "    corpus_es = processor_es.clean(corpus_es)\n",
    "    pd.DataFrame(corpus_es).to_csv(\"data/corpus_es.csv\", index=False)\n",
    "\n",
    "corpus_en = pd.read_csv(\"data/corpus_en.csv\")    \n",
    "corpus_es = pd.read_csv(\"data/corpus_es.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploramos los textos para cada idioma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Resumption of the session</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I declare resumed the session of the European ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0                          Resumption of the session\n",
       "1  I declare resumed the session of the European ..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_en[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Reanudación del período de sesiones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Declaro reanudado el período de sesiones del P...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0                Reanudación del período de sesiones\n",
       "1  Declaro reanudado el período de sesiones del P..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_es[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1965735, 1965735)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_en), len(corpus_es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s-Y9v8-Tozl2"
   },
   "source": [
    "## Tokenizar el Texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizado del texto sin aplicar limpieza (aplicada en el apartado anterior) y sin padding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('data/processor_en.joblib'):\n",
    "    tokens_en = processor_en.process_text(corpus_en, \n",
    "                                             isclean=True, \n",
    "                                             padding=False)\n",
    "    dump(processor_en, 'data/processor_en.joblib')\n",
    "else:\n",
    "    processor_en = load('data/processor_en.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('data/processor_es.joblib'):\n",
    "    tokens_es = processor_es.process_text(corpus_es, \n",
    "                                             isclean=True, \n",
    "                                             padding=False)\n",
    "    dump(processor_es, 'data/processor_es.joblib')\n",
    "else:\n",
    "    processor_es = load('data/processor_es.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tamaño de Vocabulario para los dos idiomas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ftIbPzIwCtwL"
   },
   "outputs": [],
   "source": [
    "if not os.path.isfile(\"data/inputs.csv\") and not os.path.isfile(\"data/outputs.csv\"):\n",
    "    VOCAB_SIZE_EN = processor_en.tokenizer.vocab_size + 2\n",
    "    VOCAB_SIZE_ES = processor_es.tokenizer.vocab_size + 2\n",
    "\n",
    "    print(VOCAB_SIZE_EN, VOCAB_SIZE_ES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sustituimos los valores NaN con valores vacios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(\"data/inputs.csv\") and not os.path.isfile(\"data/outputs.csv\"):\n",
    "    corpus_es = corpus_es.fillna(\" \")\n",
    "    corpus_en = corpus_en.fillna(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparación de las frases como inputs/outputs del Modelo:\n",
    "\n",
    "> _**[ \\INICIO ]**_ + frase + _**[ \\FIN ]**_\n",
    "\n",
    "- **[ \\INICIO ]**: Carácter que determina el inicio de frase.\n",
    "- **[ \\FIN ]**: Carácter que determina el final de frase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oPFe2YJDC9jw"
   },
   "outputs": [],
   "source": [
    "if not os.path.isfile(\"data/inputs.csv\") and not os.path.isfile(\"data/outputs.csv\"):\n",
    "    inputs = [[VOCAB_SIZE_EN-2] + \\\n",
    "              processor_en.tokenizer.encode(sentence[0]) + [VOCAB_SIZE_EN-1] \\\n",
    "                for sentence in corpus_en.values]\n",
    "\n",
    "    outputs = [[VOCAB_SIZE_ES-2] + \\\n",
    "               processor_es.tokenizer.encode(sentence[0]) + [VOCAB_SIZE_ES-1] \n",
    "                for sentence in corpus_es.values ]\n",
    "    len(inputs), len(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bG6AlcFMpC5C"
   },
   "source": [
    "## Eliminamos las frases demasiado largas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F6CD6PLGyQWy"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 20\n",
    "\n",
    "if not os.path.isfile(\"data/inputs.csv\") and not os.path.isfile(\"data/outputs.csv\"):\n",
    "    idx_to_remove = [count for count, sent in enumerate(inputs)\n",
    "                     if len(sent) > MAX_LENGTH]\n",
    "    if len(idx_to_remove) > 0:\n",
    "        for idx in reversed(idx_to_remove):\n",
    "            del inputs[idx]\n",
    "            del outputs[idx]\n",
    "\n",
    "    idx_to_remove = [count for count, sent in enumerate(outputs)\n",
    "                     if len(sent) > MAX_LENGTH]\n",
    "    if len(idx_to_remove) > 0:\n",
    "        for idx in reversed(idx_to_remove):\n",
    "            del inputs[idx]\n",
    "            del outputs[idx]\n",
    "\n",
    "    pd.DataFrame(inputs).to_csv(\"data/inputs.csv\", index=False)\n",
    "    pd.DataFrame(outputs).to_csv(\"data/outputs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ypm8h5aZQTZ1"
   },
   "source": [
    "## Creamos las entradas y las salidas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9FP0WPsdM8hl"
   },
   "source": [
    "A medida que entrenamos con bloques, necesitaremos que cada entrada tenga la misma longitud. Rellenamos con el token apropiado, y nos aseguraremos de que este token de relleno no interfiera con nuestro entrenamiento más adelante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(411131, 411131)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = pd.read_csv(\"data/inputs.csv\").fillna(0).astype(int)   \n",
    "outputs = pd.read_csv(\"data/outputs.csv\").fillna(0).astype(int)\n",
    "\n",
    "len(inputs), len(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nvDfLDWUONlE"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 20\n",
    "\n",
    "VOCAB_SIZE_EN = 8198\n",
    "VOCAB_SIZE_ES = 8225 \n",
    "\n",
    "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs.values,\n",
    "                                                       value=0,\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=MAX_LENGTH)\n",
    "outputs = tf.keras.preprocessing.sequence.pad_sequences(outputs.values,\n",
    "                                                        value=0,\n",
    "                                                        padding='post',\n",
    "                                                        maxlen=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea el daset generador para servir los inputs/outputs procesados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wFxMp3TOIYff"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-c-LRThUPrso"
   },
   "source": [
    "## Modelo Transformer - Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qiOdqQ5qPs8z"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Hiper Parámetros\n",
    "D_MODEL = 128 # 512\n",
    "NB_LAYERS = 4 # 6\n",
    "FFN_UNITS = 512 # 2048\n",
    "NB_PROJ = 8 # 8\n",
    "DROPOUT_RATE = 0.1 # 0.1\n",
    "\n",
    "model_Transformer = Transformer(vocab_size_enc=VOCAB_SIZE_EN,\n",
    "                          vocab_size_dec=VOCAB_SIZE_ES,\n",
    "                          d_model=D_MODEL,\n",
    "                          nb_layers=NB_LAYERS,\n",
    "                          FFN_units=FFN_UNITS,\n",
    "                          nb_proj=NB_PROJ,\n",
    "                          dropout_rate=DROPOUT_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bucle de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlearner.nlp import Transformer_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The last checkpoint has been restored\n",
      "Inicio del epoch 1\n",
      "Epoch 1 Lote 0 Pérdida 1.5783 Precisión 0.3618\n",
      "Epoch 1 Lote 50 Pérdida 1.7407 Precisión 0.3671\n",
      "Epoch 1 Lote 100 Pérdida 1.7100 Precisión 0.3697\n",
      "Epoch 1 Lote 150 Pérdida 1.7067 Precisión 0.3718\n",
      "Epoch 1 Lote 200 Pérdida 1.7002 Precisión 0.3720\n",
      "Epoch 1 Lote 250 Pérdida 1.6966 Precisión 0.3723\n",
      "Epoch 1 Lote 300 Pérdida 1.6790 Precisión 0.3728\n",
      "Epoch 1 Lote 350 Pérdida 1.6729 Precisión 0.3734\n",
      "Epoch 1 Lote 400 Pérdida 1.6736 Precisión 0.3736\n",
      "Epoch 1 Lote 450 Pérdida 1.6689 Precisión 0.3737\n",
      "Epoch 1 Lote 500 Pérdida 1.6622 Precisión 0.3741\n",
      "Epoch 1 Lote 550 Pérdida 1.6596 Precisión 0.3746\n",
      "Epoch 1 Lote 600 Pérdida 1.6548 Precisión 0.3749\n",
      "Epoch 1 Lote 650 Pérdida 1.6513 Precisión 0.3754\n",
      "Epoch 1 Lote 700 Pérdida 1.6488 Precisión 0.3758\n",
      "Epoch 1 Lote 750 Pérdida 1.6461 Precisión 0.3768\n",
      "Epoch 1 Lote 800 Pérdida 1.6406 Precisión 0.3777\n",
      "Epoch 1 Lote 850 Pérdida 1.6353 Precisión 0.3789\n",
      "Epoch 1 Lote 900 Pérdida 1.6284 Precisión 0.3800\n",
      "Epoch 1 Lote 950 Pérdida 1.6208 Precisión 0.3812\n",
      "Epoch 1 Lote 1000 Pérdida 1.6135 Precisión 0.3823\n",
      "Epoch 1 Lote 1050 Pérdida 1.6064 Precisión 0.3833\n",
      "Epoch 1 Lote 1100 Pérdida 1.5984 Precisión 0.3843\n",
      "Epoch 1 Lote 1150 Pérdida 1.5916 Precisión 0.3851\n",
      "Epoch 1 Lote 1200 Pérdida 1.5840 Precisión 0.3859\n",
      "Epoch 1 Lote 1250 Pérdida 1.5773 Precisión 0.3868\n",
      "Epoch 1 Lote 1300 Pérdida 1.5693 Precisión 0.3878\n",
      "Epoch 1 Lote 1350 Pérdida 1.5619 Precisión 0.3885\n",
      "Epoch 1 Lote 1400 Pérdida 1.5541 Precisión 0.3895\n",
      "Epoch 1 Lote 1450 Pérdida 1.5478 Precisión 0.3905\n",
      "Epoch 1 Lote 1500 Pérdida 1.5416 Precisión 0.3915\n",
      "Epoch 1 Lote 1550 Pérdida 1.5354 Precisión 0.3924\n",
      "Epoch 1 Lote 1600 Pérdida 1.5296 Precisión 0.3932\n",
      "Epoch 1 Lote 1650 Pérdida 1.5217 Precisión 0.3940\n",
      "Epoch 1 Lote 1700 Pérdida 1.5168 Precisión 0.3950\n",
      "Epoch 1 Lote 1750 Pérdida 1.5111 Precisión 0.3958\n",
      "Epoch 1 Lote 1800 Pérdida 1.5050 Precisión 0.3965\n",
      "Epoch 1 Lote 1850 Pérdida 1.4995 Precisión 0.3972\n",
      "Epoch 1 Lote 1900 Pérdida 1.4947 Precisión 0.3977\n",
      "Epoch 1 Lote 1950 Pérdida 1.4893 Precisión 0.3982\n",
      "Epoch 1 Lote 2000 Pérdida 1.4839 Precisión 0.3987\n",
      "Epoch 1 Lote 2050 Pérdida 1.4786 Precisión 0.3992\n",
      "Epoch 1 Lote 2100 Pérdida 1.4735 Precisión 0.3997\n",
      "Epoch 1 Lote 2150 Pérdida 1.4679 Precisión 0.4002\n",
      "Epoch 1 Lote 2200 Pérdida 1.4629 Precisión 0.4009\n",
      "Epoch 1 Lote 2250 Pérdida 1.4580 Precisión 0.4014\n",
      "Epoch 1 Lote 2300 Pérdida 1.4537 Precisión 0.4020\n",
      "Epoch 1 Lote 2350 Pérdida 1.4488 Precisión 0.4026\n",
      "Epoch 1 Lote 2400 Pérdida 1.4444 Precisión 0.4031\n",
      "Epoch 1 Lote 2450 Pérdida 1.4384 Precisión 0.4037\n",
      "Epoch 1 Lote 2500 Pérdida 1.4334 Precisión 0.4044\n",
      "Epoch 1 Lote 2550 Pérdida 1.4288 Precisión 0.4049\n",
      "Epoch 1 Lote 2600 Pérdida 1.4238 Precisión 0.4054\n",
      "Epoch 1 Lote 2650 Pérdida 1.4189 Precisión 0.4059\n",
      "Epoch 1 Lote 2700 Pérdida 1.4146 Precisión 0.4065\n",
      "Epoch 1 Lote 2750 Pérdida 1.4103 Precisión 0.4070\n",
      "Epoch 1 Lote 2800 Pérdida 1.4072 Precisión 0.4075\n",
      "Epoch 1 Lote 2850 Pérdida 1.4034 Precisión 0.4080\n",
      "Epoch 1 Lote 2900 Pérdida 1.3999 Precisión 0.4085\n",
      "Epoch 1 Lote 2950 Pérdida 1.3965 Precisión 0.4091\n",
      "Epoch 1 Lote 3000 Pérdida 1.3935 Precisión 0.4096\n",
      "Epoch 1 Lote 3050 Pérdida 1.3902 Precisión 0.4102\n",
      "Epoch 1 Lote 3100 Pérdida 1.3870 Precisión 0.4107\n",
      "Epoch 1 Lote 3150 Pérdida 1.3842 Precisión 0.4112\n",
      "Epoch 1 Lote 3200 Pérdida 1.3817 Precisión 0.4117\n",
      "Epoch 1 Lote 3250 Pérdida 1.3790 Precisión 0.4122\n",
      "Epoch 1 Lote 3300 Pérdida 1.3762 Precisión 0.4128\n",
      "Epoch 1 Lote 3350 Pérdida 1.3731 Precisión 0.4133\n",
      "Epoch 1 Lote 3400 Pérdida 1.3702 Precisión 0.4139\n",
      "Epoch 1 Lote 3450 Pérdida 1.3674 Precisión 0.4144\n",
      "Epoch 1 Lote 3500 Pérdida 1.3648 Precisión 0.4149\n",
      "Epoch 1 Lote 3550 Pérdida 1.3620 Precisión 0.4155\n",
      "Epoch 1 Lote 3600 Pérdida 1.3594 Precisión 0.4160\n",
      "Epoch 1 Lote 3650 Pérdida 1.3567 Precisión 0.4165\n",
      "Epoch 1 Lote 3700 Pérdida 1.3541 Precisión 0.4170\n",
      "Epoch 1 Lote 3750 Pérdida 1.3511 Precisión 0.4175\n",
      "Epoch 1 Lote 3800 Pérdida 1.3483 Precisión 0.4180\n",
      "Epoch 1 Lote 3850 Pérdida 1.3457 Precisión 0.4185\n",
      "Epoch 1 Lote 3900 Pérdida 1.3429 Precisión 0.4189\n",
      "Epoch 1 Lote 3950 Pérdida 1.3406 Precisión 0.4194\n",
      "Epoch 1 Lote 4000 Pérdida 1.3382 Precisión 0.4199\n",
      "Epoch 1 Lote 4050 Pérdida 1.3358 Precisión 0.4205\n",
      "Epoch 1 Lote 4100 Pérdida 1.3332 Precisión 0.4210\n",
      "Epoch 1 Lote 4150 Pérdida 1.3302 Precisión 0.4215\n",
      "Epoch 1 Lote 4200 Pérdida 1.3275 Precisión 0.4221\n",
      "Epoch 1 Lote 4250 Pérdida 1.3249 Precisión 0.4226\n",
      "Epoch 1 Lote 4300 Pérdida 1.3224 Precisión 0.4232\n",
      "Epoch 1 Lote 4350 Pérdida 1.3197 Precisión 0.4238\n",
      "Epoch 1 Lote 4400 Pérdida 1.3170 Precisión 0.4244\n",
      "Epoch 1 Lote 4450 Pérdida 1.3148 Precisión 0.4249\n",
      "Epoch 1 Lote 4500 Pérdida 1.3123 Precisión 0.4255\n",
      "Epoch 1 Lote 4550 Pérdida 1.3098 Precisión 0.4260\n",
      "Epoch 1 Lote 4600 Pérdida 1.3076 Precisión 0.4265\n",
      "Epoch 1 Lote 4650 Pérdida 1.3054 Precisión 0.4271\n",
      "Epoch 1 Lote 4700 Pérdida 1.3039 Precisión 0.4274\n",
      "Epoch 1 Lote 4750 Pérdida 1.3030 Precisión 0.4277\n",
      "Epoch 1 Lote 4800 Pérdida 1.3028 Precisión 0.4279\n",
      "Epoch 1 Lote 4850 Pérdida 1.3029 Precisión 0.4280\n",
      "Epoch 1 Lote 4900 Pérdida 1.3032 Precisión 0.4280\n",
      "Epoch 1 Lote 4950 Pérdida 1.3034 Precisión 0.4280\n",
      "Epoch 1 Lote 5000 Pérdida 1.3042 Precisión 0.4279\n",
      "Epoch 1 Lote 5050 Pérdida 1.3052 Precisión 0.4278\n",
      "Epoch 1 Lote 5100 Pérdida 1.3061 Precisión 0.4277\n",
      "Epoch 1 Lote 5150 Pérdida 1.3070 Precisión 0.4276\n",
      "Epoch 1 Lote 5200 Pérdida 1.3083 Precisión 0.4275\n",
      "Epoch 1 Lote 5250 Pérdida 1.3095 Precisión 0.4274\n",
      "Epoch 1 Lote 5300 Pérdida 1.3111 Precisión 0.4272\n",
      "Epoch 1 Lote 5350 Pérdida 1.3126 Precisión 0.4271\n",
      "Epoch 1 Lote 5400 Pérdida 1.3136 Precisión 0.4269\n",
      "Epoch 1 Lote 5450 Pérdida 1.3147 Precisión 0.4268\n",
      "Epoch 1 Lote 5500 Pérdida 1.3161 Precisión 0.4267\n",
      "Epoch 1 Lote 5550 Pérdida 1.3168 Precisión 0.4266\n",
      "Epoch 1 Lote 5600 Pérdida 1.3176 Precisión 0.4264\n",
      "Epoch 1 Lote 5650 Pérdida 1.3183 Precisión 0.4263\n",
      "Epoch 1 Lote 5700 Pérdida 1.3193 Precisión 0.4262\n",
      "Epoch 1 Lote 5750 Pérdida 1.3204 Precisión 0.4261\n",
      "Epoch 1 Lote 5800 Pérdida 1.3212 Precisión 0.4259\n",
      "Epoch 1 Lote 5850 Pérdida 1.3221 Precisión 0.4257\n",
      "Epoch 1 Lote 5900 Pérdida 1.3232 Precisión 0.4256\n",
      "Epoch 1 Lote 5950 Pérdida 1.3242 Precisión 0.4254\n",
      "Epoch 1 Lote 6000 Pérdida 1.3249 Precisión 0.4252\n",
      "Epoch 1 Lote 6050 Pérdida 1.3260 Precisión 0.4250\n",
      "Epoch 1 Lote 6100 Pérdida 1.3267 Precisión 0.4249\n",
      "Epoch 1 Lote 6150 Pérdida 1.3275 Precisión 0.4247\n",
      "Epoch 1 Lote 6200 Pérdida 1.3280 Precisión 0.4246\n",
      "Epoch 1 Lote 6250 Pérdida 1.3285 Precisión 0.4244\n",
      "Epoch 1 Lote 6300 Pérdida 1.3290 Precisión 0.4243\n",
      "Epoch 1 Lote 6350 Pérdida 1.3292 Precisión 0.4242\n",
      "Epoch 1 Lote 6400 Pérdida 1.3298 Precisión 0.4241\n",
      "Guardando checkpoint para el epoch 1 en ckpt/ckpt-2\n",
      "Tiempo que ha tardado 1 epoch: 3027.705971956253 segs\n",
      "\n",
      "Inicio del epoch 2\n",
      "Epoch 2 Lote 0 Pérdida 1.4140 Precisión 0.4293\n",
      "Epoch 2 Lote 50 Pérdida 1.3890 Precisión 0.4086\n",
      "Epoch 2 Lote 100 Pérdida 1.3908 Precisión 0.4121\n",
      "Epoch 2 Lote 150 Pérdida 1.3905 Precisión 0.4131\n",
      "Epoch 2 Lote 200 Pérdida 1.3894 Precisión 0.4139\n",
      "Epoch 2 Lote 250 Pérdida 1.3876 Precisión 0.4140\n",
      "Epoch 2 Lote 300 Pérdida 1.3860 Precisión 0.4137\n",
      "Epoch 2 Lote 350 Pérdida 1.3844 Precisión 0.4143\n",
      "Epoch 2 Lote 400 Pérdida 1.3817 Precisión 0.4157\n",
      "Epoch 2 Lote 450 Pérdida 1.3808 Precisión 0.4161\n",
      "Epoch 2 Lote 500 Pérdida 1.3734 Precisión 0.4163\n",
      "Epoch 2 Lote 550 Pérdida 1.3706 Precisión 0.4167\n",
      "Epoch 2 Lote 600 Pérdida 1.3689 Precisión 0.4168\n",
      "Epoch 2 Lote 650 Pérdida 1.3683 Precisión 0.4158\n",
      "Epoch 2 Lote 700 Pérdida 1.3667 Precisión 0.4162\n",
      "Epoch 2 Lote 750 Pérdida 1.3624 Precisión 0.4166\n",
      "Epoch 2 Lote 800 Pérdida 1.3594 Precisión 0.4175\n",
      "Epoch 2 Lote 850 Pérdida 1.3529 Precisión 0.4187\n",
      "Epoch 2 Lote 900 Pérdida 1.3459 Precisión 0.4198\n",
      "Epoch 2 Lote 950 Pérdida 1.3395 Precisión 0.4209\n",
      "Epoch 2 Lote 1000 Pérdida 1.3326 Precisión 0.4221\n",
      "Epoch 2 Lote 1050 Pérdida 1.3254 Precisión 0.4233\n",
      "Epoch 2 Lote 1100 Pérdida 1.3186 Precisión 0.4242\n",
      "Epoch 2 Lote 1150 Pérdida 1.3129 Precisión 0.4250\n",
      "Epoch 2 Lote 1200 Pérdida 1.3069 Precisión 0.4256\n",
      "Epoch 2 Lote 1250 Pérdida 1.3010 Precisión 0.4261\n",
      "Epoch 2 Lote 1300 Pérdida 1.2963 Precisión 0.4268\n",
      "Epoch 2 Lote 1350 Pérdida 1.2896 Precisión 0.4277\n",
      "Epoch 2 Lote 1400 Pérdida 1.2848 Precisión 0.4286\n",
      "Epoch 2 Lote 1450 Pérdida 1.2790 Precisión 0.4294\n",
      "Epoch 2 Lote 1500 Pérdida 1.2732 Precisión 0.4301\n",
      "Epoch 2 Lote 1550 Pérdida 1.2688 Precisión 0.4309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Lote 1600 Pérdida 1.2645 Precisión 0.4316\n",
      "Epoch 2 Lote 1650 Pérdida 1.2592 Precisión 0.4324\n",
      "Epoch 2 Lote 1700 Pérdida 1.2540 Precisión 0.4331\n",
      "Epoch 2 Lote 1750 Pérdida 1.2494 Precisión 0.4338\n",
      "Epoch 2 Lote 1800 Pérdida 1.2446 Precisión 0.4343\n",
      "Epoch 2 Lote 1850 Pérdida 1.2396 Precisión 0.4348\n",
      "Epoch 2 Lote 1900 Pérdida 1.2349 Precisión 0.4351\n",
      "Epoch 2 Lote 1950 Pérdida 1.2308 Precisión 0.4355\n",
      "Epoch 2 Lote 2000 Pérdida 1.2262 Precisión 0.4360\n",
      "Epoch 2 Lote 2050 Pérdida 1.2218 Precisión 0.4364\n",
      "Epoch 2 Lote 2100 Pérdida 1.2177 Precisión 0.4368\n",
      "Epoch 2 Lote 2150 Pérdida 1.2129 Precisión 0.4373\n",
      "Epoch 2 Lote 2200 Pérdida 1.2094 Precisión 0.4378\n",
      "Epoch 2 Lote 2250 Pérdida 1.2055 Precisión 0.4381\n",
      "Epoch 2 Lote 2300 Pérdida 1.2019 Precisión 0.4385\n",
      "Epoch 2 Lote 2350 Pérdida 1.1979 Precisión 0.4389\n",
      "Epoch 2 Lote 2400 Pérdida 1.1941 Precisión 0.4395\n",
      "Epoch 2 Lote 2450 Pérdida 1.1903 Precisión 0.4400\n",
      "Epoch 2 Lote 2500 Pérdida 1.1869 Precisión 0.4404\n",
      "Epoch 2 Lote 2550 Pérdida 1.1826 Precisión 0.4408\n",
      "Epoch 2 Lote 2600 Pérdida 1.1792 Precisión 0.4412\n",
      "Epoch 2 Lote 2650 Pérdida 1.1759 Precisión 0.4415\n",
      "Epoch 2 Lote 2700 Pérdida 1.1731 Precisión 0.4418\n",
      "Epoch 2 Lote 2750 Pérdida 1.1701 Precisión 0.4423\n",
      "Epoch 2 Lote 2800 Pérdida 1.1672 Precisión 0.4427\n",
      "Epoch 2 Lote 2850 Pérdida 1.1646 Precisión 0.4430\n",
      "Epoch 2 Lote 2900 Pérdida 1.1623 Precisión 0.4434\n",
      "Epoch 2 Lote 2950 Pérdida 1.1605 Precisión 0.4438\n",
      "Epoch 2 Lote 3000 Pérdida 1.1580 Precisión 0.4442\n",
      "Epoch 2 Lote 3050 Pérdida 1.1558 Precisión 0.4446\n",
      "Epoch 2 Lote 3100 Pérdida 1.1537 Precisión 0.4449\n",
      "Epoch 2 Lote 3150 Pérdida 1.1515 Precisión 0.4453\n",
      "Epoch 2 Lote 3200 Pérdida 1.1494 Precisión 0.4457\n",
      "Epoch 2 Lote 3250 Pérdida 1.1478 Precisión 0.4460\n",
      "Epoch 2 Lote 3300 Pérdida 1.1460 Precisión 0.4465\n",
      "Epoch 2 Lote 3350 Pérdida 1.1443 Precisión 0.4469\n",
      "Epoch 2 Lote 3400 Pérdida 1.1422 Precisión 0.4473\n",
      "Epoch 2 Lote 3450 Pérdida 1.1411 Precisión 0.4477\n",
      "Epoch 2 Lote 3500 Pérdida 1.1395 Precisión 0.4481\n",
      "Epoch 2 Lote 3550 Pérdida 1.1379 Precisión 0.4485\n",
      "Epoch 2 Lote 3600 Pérdida 1.1361 Precisión 0.4489\n",
      "Epoch 2 Lote 3650 Pérdida 1.1341 Precisión 0.4493\n",
      "Epoch 2 Lote 3700 Pérdida 1.1323 Precisión 0.4497\n",
      "Epoch 2 Lote 3750 Pérdida 1.1309 Precisión 0.4500\n",
      "Epoch 2 Lote 3800 Pérdida 1.1289 Precisión 0.4505\n",
      "Epoch 2 Lote 3850 Pérdida 1.1273 Precisión 0.4508\n",
      "Epoch 2 Lote 3900 Pérdida 1.1253 Precisión 0.4512\n",
      "Epoch 2 Lote 3950 Pérdida 1.1236 Precisión 0.4515\n",
      "Epoch 2 Lote 4000 Pérdida 1.1218 Precisión 0.4518\n",
      "Epoch 2 Lote 4050 Pérdida 1.1201 Precisión 0.4522\n",
      "Epoch 2 Lote 4100 Pérdida 1.1184 Precisión 0.4526\n",
      "Epoch 2 Lote 4150 Pérdida 1.1165 Precisión 0.4531\n",
      "Epoch 2 Lote 4200 Pérdida 1.1145 Precisión 0.4535\n",
      "Epoch 2 Lote 4250 Pérdida 1.1126 Precisión 0.4540\n",
      "Epoch 2 Lote 4300 Pérdida 1.1107 Precisión 0.4545\n",
      "Epoch 2 Lote 4350 Pérdida 1.1091 Precisión 0.4549\n",
      "Epoch 2 Lote 4400 Pérdida 1.1075 Precisión 0.4554\n",
      "Epoch 2 Lote 4450 Pérdida 1.1057 Precisión 0.4559\n",
      "Epoch 2 Lote 4500 Pérdida 1.1040 Precisión 0.4563\n",
      "Epoch 2 Lote 4550 Pérdida 1.1025 Precisión 0.4567\n",
      "Epoch 2 Lote 4600 Pérdida 1.1010 Precisión 0.4572\n",
      "Epoch 2 Lote 4650 Pérdida 1.0996 Precisión 0.4576\n",
      "Epoch 2 Lote 4700 Pérdida 1.0990 Precisión 0.4578\n",
      "Epoch 2 Lote 4750 Pérdida 1.0989 Precisión 0.4580\n",
      "Epoch 2 Lote 4800 Pérdida 1.0995 Precisión 0.4581\n",
      "Epoch 2 Lote 4850 Pérdida 1.1000 Precisión 0.4580\n",
      "Epoch 2 Lote 4900 Pérdida 1.1011 Precisión 0.4580\n",
      "Epoch 2 Lote 4950 Pérdida 1.1025 Precisión 0.4578\n",
      "Epoch 2 Lote 5000 Pérdida 1.1039 Precisión 0.4576\n",
      "Epoch 2 Lote 5050 Pérdida 1.1053 Precisión 0.4574\n",
      "Epoch 2 Lote 5100 Pérdida 1.1069 Precisión 0.4572\n",
      "Epoch 2 Lote 5150 Pérdida 1.1091 Precisión 0.4570\n",
      "Epoch 2 Lote 5200 Pérdida 1.1109 Precisión 0.4568\n",
      "Epoch 2 Lote 5250 Pérdida 1.1127 Precisión 0.4565\n",
      "Epoch 2 Lote 5300 Pérdida 1.1145 Precisión 0.4563\n",
      "Epoch 2 Lote 5350 Pérdida 1.1164 Precisión 0.4561\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 2\n",
    "\n",
    "Transformer_train(model_Transformer,\n",
    "                  dataset,\n",
    "                  d_model=D_MODEL,\n",
    "                  train=TRAIN,\n",
    "                  epochs=EPOCHS,\n",
    "                  checkpoint_path=\"ckpt/\",\n",
    "                  max_to_keep=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nmzyRwDrRGdq"
   },
   "source": [
    "## Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cNHwJJrz3lPB"
   },
   "outputs": [],
   "source": [
    "def evaluate(inp_sentence):\n",
    "    inp_sentence = \\\n",
    "        [VOCAB_SIZE_EN-2] + processor_en.tokenizer.encode(inp_sentence) + [VOCAB_SIZE_EN-1]\n",
    "    enc_input = tf.expand_dims(inp_sentence, axis=0)\n",
    "    \n",
    "    output = tf.expand_dims([VOCAB_SIZE_ES-2], axis=0)\n",
    "    \n",
    "    for _ in range(MAX_LENGTH):\n",
    "        predictions = model_Transformer(enc_input, output, False) #(1, seq_length, VOCAB_SIZE_ES)\n",
    "        \n",
    "        prediction = predictions[:, -1:, :]\n",
    "        \n",
    "        predicted_id = tf.cast(tf.argmax(prediction, axis=-1), tf.int32)\n",
    "        \n",
    "        if predicted_id == VOCAB_SIZE_ES-1:\n",
    "            return tf.squeeze(output, axis=0)\n",
    "        \n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "        \n",
    "    return tf.squeeze(output, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s6VeFKrE6Kdx"
   },
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    output = evaluate(sentence).numpy()\n",
    "    \n",
    "    predicted_sentence = processor_es.tokenizer.decode(\n",
    "        [i for i in output if i < VOCAB_SIZE_ES-2]\n",
    "    )\n",
    "    \n",
    "    print(\"Entrada: {}\".format(sentence))\n",
    "    print(\"Traducción predicha: {}\".format(predicted_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(\"I have got a house\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "BupFjJlgDvCA",
    "outputId": "54e7aa51-1cc2-42a5-ede2-304d1a230d08"
   },
   "outputs": [],
   "source": [
    "translate(\"This is a problem we have to solve.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "ZdoWKbCP7Czs",
    "outputId": "dbe5425f-1c91-45a7-87ab-4ed0dbf13d74"
   },
   "outputs": [],
   "source": [
    "translate(\"This is a really powerful tool!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "UGjBEb5WFMGt",
    "outputId": "a7892348-eaf4-4445-d816-3c587275f436"
   },
   "outputs": [],
   "source": [
    "translate(\"This is an interesting course about Natural Language Processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L1v9g4T0FcO3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Transformer para NLP.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
