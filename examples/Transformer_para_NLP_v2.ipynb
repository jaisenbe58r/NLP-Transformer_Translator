{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRANSFORMER para la Traducción de Texto\n",
    "\n",
    "> Basado en:https://www.udemy.com/course/procesamiento-del-lenguaje-natural/learn/lecture/21502260#overview\n",
    "\n",
    "- Author: Juan Gabriel Gomila\n",
    "- Course: \"Procesamiento del Lenguaje Natural Moderno en Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m9JJ7FBw84tG"
   },
   "source": [
    "## Importar las dependencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t5DbIHC-F6Hf"
   },
   "source": [
    "**Paper original**: All you need is Attention https://arxiv.org/pdf/1706.03762.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZbcvtPlp3YWu"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P6o_cpZz3y_-"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    %tensorflow_version 2.x\n",
    "except:\n",
    "    pass\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlearner.nlp import Transformer\n",
    "from mlearner.nlp import Processor_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BQN8jwx48_yU"
   },
   "source": [
    "## Pre Procesado de Datos\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bPlOT-2mlw0r"
   },
   "source": [
    "## Carga de Ficheros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q8Or0sLV5b8t"
   },
   "outputs": [],
   "source": [
    "with open(\"data/europarl-v7.es-en.en\", \n",
    "          mode = \"r\", encoding = \"utf-8\") as f:\n",
    "    europarl_en = f.read()\n",
    "with open(\"data/europarl-v7.es-en.es\", \n",
    "          mode = \"r\", encoding = \"utf-8\") as f:\n",
    "    europarl_es = f.read()\n",
    "with open(\"data/P85-Non-Breaking-Prefix.en\", \n",
    "          mode = \"r\", encoding = \"utf-8\") as f:\n",
    "    non_breaking_prefix_en = f.read()\n",
    "with open(\"data/P85-Non-Breaking-Prefix.en\", \n",
    "          mode = \"r\", encoding = \"utf-8\") as f:\n",
    "    non_breaking_prefix_es = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 58
    },
    "colab_type": "code",
    "id": "TMAFFdpIyNZd",
    "outputId": "eb684c33-6895-4fc9-a27d-420c05a5fab8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Resumption of the session\\nI declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive peri'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "europarl_en[:230]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 58
    },
    "colab_type": "code",
    "id": "BYgCMq6myYIi",
    "outputId": "f76b75fd-25df-43f2-fe4b-f88de485e69d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reanudación del período de sesiones\\nDeclaro reanudado el período de sesiones del Parlamento Europeo, interrumpido el viernes 17 de diciembre pasado, y reitero a Sus Señorías mi deseo de que hayan tenido unas buenas vacaciones'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "europarl_es[:225]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TEFw0D2vP_Dl"
   },
   "source": [
    "## Limpieza de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos funcion de procesado de texto basada en expresiones regulares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Function_clean(text):\n",
    "    \n",
    "    # Eliminamos la @ y su mención\n",
    "    text = re.sub(r\"@[A-Za-z0-9]+\", ' ', text)\n",
    "    # Eliminamos los links de las URLs\n",
    "    text = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se procesan los textos para cada uno de los idiomas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor_en = Processor_data(target_vocab_size=2**13, \n",
    "                              language=\"en\", \n",
    "                              function = Function_clean,\n",
    "                              name=\"processor_en\",\n",
    "                             )\n",
    "processor_es = Processor_data(target_vocab_size=2**13, \n",
    "                              language=\"es\", \n",
    "                              function = Function_clean,\n",
    "                              name=\"processor_es\"\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(\"data/corpus_en.csv\"):\n",
    "    corpus_en = europarl_en\n",
    "    corpus_en = processor_en.clean(corpus_en)\n",
    "    pd.DataFrame(corpus_en).to_csv(\"data/corpus_en.csv\", index=False)\n",
    "\n",
    "if not os.path.isfile(\"data/corpus_es.csv\"):\n",
    "    corpus_es = europarl_es\n",
    "    corpus_es = processor_es.clean(corpus_es)\n",
    "    pd.DataFrame(corpus_es).to_csv(\"data/corpus_es.csv\", index=False)\n",
    "\n",
    "corpus_en = pd.read_csv(\"data/corpus_en.csv\")    \n",
    "corpus_es = pd.read_csv(\"data/corpus_es.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploramos los textos para cada idioma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Resumption of the session</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I declare resumed the session of the European ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0                          Resumption of the session\n",
       "1  I declare resumed the session of the European ..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_en[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Reanudación del período de sesiones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Declaro reanudado el período de sesiones del P...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0                Reanudación del período de sesiones\n",
       "1  Declaro reanudado el período de sesiones del P..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_es[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1965735, 1965735)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_en), len(corpus_es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s-Y9v8-Tozl2"
   },
   "source": [
    "## Tokenizar el Texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizado del texto sin aplicar limpieza (aplicada en el apartado anterior) y sin padding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('data/processor_en.joblib'):\n",
    "    tokens_en = processor_en.process_text(corpus_en, \n",
    "                                             isclean=True, \n",
    "                                             padding=False)\n",
    "    dump(processor_en, 'data/processor_en.joblib')\n",
    "else:\n",
    "    processor_en = load('data/processor_en.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('data/processor_es.joblib'):\n",
    "    tokens_es = processor_es.process_text(corpus_es, \n",
    "                                             isclean=True, \n",
    "                                             padding=False)\n",
    "    dump(processor_es, 'data/processor_es.joblib')\n",
    "else:\n",
    "    processor_es = load('data/processor_es.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tamaño de Vocabulario para los dos idiomas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ftIbPzIwCtwL"
   },
   "outputs": [],
   "source": [
    "if not os.path.isfile(\"data/inputs.csv\") and not os.path.isfile(\"data/outputs.csv\"):\n",
    "    VOCAB_SIZE_EN = processor_en.tokenizer.vocab_size + 2\n",
    "    VOCAB_SIZE_ES = processor_es.tokenizer.vocab_size + 2\n",
    "\n",
    "    print(VOCAB_SIZE_EN, VOCAB_SIZE_ES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sustituimos los valores NaN con valores vacios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(\"data/inputs.csv\") and not os.path.isfile(\"data/outputs.csv\"):\n",
    "    corpus_es = corpus_es.fillna(\" \")\n",
    "    corpus_en = corpus_en.fillna(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparación de las frases como inputs/outputs del Modelo:\n",
    "\n",
    "> _**[ \\INICIO ]**_ + frase + _**[ \\FIN ]**_\n",
    "\n",
    "- **[ \\INICIO ]**: Carácter que determina el inicio de frase.\n",
    "- **[ \\FIN ]**: Carácter que determina el final de frase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oPFe2YJDC9jw"
   },
   "outputs": [],
   "source": [
    "if not os.path.isfile(\"data/inputs.csv\") and not os.path.isfile(\"data/outputs.csv\"):\n",
    "    inputs = [[VOCAB_SIZE_EN-2] + \\\n",
    "              processor_en.tokenizer.encode(sentence[0]) + [VOCAB_SIZE_EN-1] \\\n",
    "                for sentence in corpus_en.values]\n",
    "\n",
    "    outputs = [[VOCAB_SIZE_ES-2] + \\\n",
    "               processor_es.tokenizer.encode(sentence[0]) + [VOCAB_SIZE_ES-1] \n",
    "                for sentence in corpus_es.values ]\n",
    "    len(inputs), len(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bG6AlcFMpC5C"
   },
   "source": [
    "## Eliminamos las frases demasiado largas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F6CD6PLGyQWy"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 20\n",
    "\n",
    "if not os.path.isfile(\"data/inputs.csv\") and not os.path.isfile(\"data/outputs.csv\"):\n",
    "    idx_to_remove = [count for count, sent in enumerate(inputs)\n",
    "                     if len(sent) > MAX_LENGTH]\n",
    "    if len(idx_to_remove) > 0:\n",
    "        for idx in reversed(idx_to_remove):\n",
    "            del inputs[idx]\n",
    "            del outputs[idx]\n",
    "\n",
    "    idx_to_remove = [count for count, sent in enumerate(outputs)\n",
    "                     if len(sent) > MAX_LENGTH]\n",
    "    if len(idx_to_remove) > 0:\n",
    "        for idx in reversed(idx_to_remove):\n",
    "            del inputs[idx]\n",
    "            del outputs[idx]\n",
    "\n",
    "    pd.DataFrame(inputs).to_csv(\"data/inputs.csv\", index=False)\n",
    "    pd.DataFrame(outputs).to_csv(\"data/outputs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ypm8h5aZQTZ1"
   },
   "source": [
    "## Creamos las entradas y las salidas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9FP0WPsdM8hl"
   },
   "source": [
    "A medida que entrenamos con bloques, necesitaremos que cada entrada tenga la misma longitud. Rellenamos con el token apropiado, y nos aseguraremos de que este token de relleno no interfiera con nuestro entrenamiento más adelante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(411131, 411131)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = pd.read_csv(\"data/inputs.csv\").fillna(0).astype(int)   \n",
    "outputs = pd.read_csv(\"data/outputs.csv\").fillna(0).astype(int)\n",
    "\n",
    "len(inputs), len(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nvDfLDWUONlE"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 20\n",
    "\n",
    "VOCAB_SIZE_EN = 8198\n",
    "VOCAB_SIZE_ES = 8225 \n",
    "\n",
    "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs.values,\n",
    "                                                       value=0,\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=MAX_LENGTH)\n",
    "outputs = tf.keras.preprocessing.sequence.pad_sequences(outputs.values,\n",
    "                                                        value=0,\n",
    "                                                        padding='post',\n",
    "                                                        maxlen=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea el daset generador para servir los inputs/outputs procesados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wFxMp3TOIYff"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-c-LRThUPrso"
   },
   "source": [
    "## Modelo Transformer - Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qiOdqQ5qPs8z"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Hiper Parámetros\n",
    "D_MODEL = 128 # 512\n",
    "NB_LAYERS = 4 # 6\n",
    "FFN_UNITS = 512 # 2048\n",
    "NB_PROJ = 8 # 8\n",
    "DROPOUT_RATE = 0.1 # 0.1\n",
    "\n",
    "model_Transformer = Transformer(vocab_size_enc=VOCAB_SIZE_EN,\n",
    "                          vocab_size_dec=VOCAB_SIZE_ES,\n",
    "                          d_model=D_MODEL,\n",
    "                          nb_layers=NB_LAYERS,\n",
    "                          FFN_units=FFN_UNITS,\n",
    "                          nb_proj=NB_PROJ,\n",
    "                          dropout_rate=DROPOUT_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bucle de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlearner.nlp import Transformer_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The last checkpoint has been restored\n",
      "Inicio del epoch 1\n",
      "Epoch 1 Lote 0 Pérdida 1.0833 Precisión 0.4350\n",
      "Epoch 1 Lote 50 Pérdida 1.1687 Precisión 0.4402\n",
      "Epoch 1 Lote 100 Pérdida 1.1845 Precisión 0.4443\n",
      "Epoch 1 Lote 150 Pérdida 1.1836 Precisión 0.4439\n",
      "Epoch 1 Lote 200 Pérdida 1.1770 Precisión 0.4449\n",
      "Epoch 1 Lote 250 Pérdida 1.1742 Precisión 0.4447\n",
      "Epoch 1 Lote 300 Pérdida 1.1768 Precisión 0.4450\n",
      "Epoch 1 Lote 350 Pérdida 1.1760 Precisión 0.4447\n",
      "Epoch 1 Lote 400 Pérdida 1.1789 Precisión 0.4453\n",
      "Epoch 1 Lote 450 Pérdida 1.1778 Precisión 0.4457\n",
      "Epoch 1 Lote 500 Pérdida 1.1764 Precisión 0.4457\n",
      "Epoch 1 Lote 550 Pérdida 1.1737 Precisión 0.4453\n",
      "Epoch 1 Lote 600 Pérdida 1.1708 Precisión 0.4457\n",
      "Epoch 1 Lote 650 Pérdida 1.1710 Precisión 0.4457\n",
      "Epoch 1 Lote 700 Pérdida 1.1677 Precisión 0.4456\n",
      "Epoch 1 Lote 750 Pérdida 1.1645 Precisión 0.4462\n",
      "Epoch 1 Lote 800 Pérdida 1.1607 Precisión 0.4471\n",
      "Epoch 1 Lote 850 Pérdida 1.1548 Precisión 0.4484\n",
      "Epoch 1 Lote 900 Pérdida 1.1491 Precisión 0.4494\n",
      "Epoch 1 Lote 950 Pérdida 1.1437 Precisión 0.4502\n",
      "Epoch 1 Lote 1000 Pérdida 1.1370 Precisión 0.4513\n",
      "Epoch 1 Lote 1050 Pérdida 1.1302 Precisión 0.4523\n",
      "Epoch 1 Lote 1100 Pérdida 1.1243 Precisión 0.4530\n",
      "Epoch 1 Lote 1150 Pérdida 1.1187 Precisión 0.4536\n",
      "Epoch 1 Lote 1200 Pérdida 1.1125 Precisión 0.4543\n",
      "Epoch 1 Lote 1250 Pérdida 1.1066 Precisión 0.4552\n",
      "Epoch 1 Lote 1300 Pérdida 1.1006 Precisión 0.4561\n",
      "Epoch 1 Lote 1350 Pérdida 1.0951 Precisión 0.4570\n",
      "Epoch 1 Lote 1400 Pérdida 1.0898 Precisión 0.4578\n",
      "Epoch 1 Lote 1450 Pérdida 1.0843 Precisión 0.4587\n",
      "Epoch 1 Lote 1500 Pérdida 1.0795 Precisión 0.4596\n",
      "Epoch 1 Lote 1550 Pérdida 1.0755 Precisión 0.4603\n",
      "Epoch 1 Lote 1600 Pérdida 1.0710 Precisión 0.4612\n",
      "Epoch 1 Lote 1650 Pérdida 1.0660 Precisión 0.4619\n",
      "Epoch 1 Lote 1700 Pérdida 1.0617 Precisión 0.4626\n",
      "Epoch 1 Lote 1750 Pérdida 1.0572 Precisión 0.4630\n",
      "Epoch 1 Lote 1800 Pérdida 1.0523 Precisión 0.4637\n",
      "Epoch 1 Lote 1850 Pérdida 1.0483 Precisión 0.4640\n",
      "Epoch 1 Lote 1900 Pérdida 1.0444 Precisión 0.4644\n",
      "Epoch 1 Lote 1950 Pérdida 1.0406 Precisión 0.4649\n",
      "Epoch 1 Lote 2000 Pérdida 1.0370 Precisión 0.4653\n",
      "Epoch 1 Lote 2050 Pérdida 1.0331 Precisión 0.4657\n",
      "Epoch 1 Lote 2100 Pérdida 1.0289 Precisión 0.4661\n",
      "Epoch 1 Lote 2150 Pérdida 1.0254 Precisión 0.4665\n",
      "Epoch 1 Lote 2200 Pérdida 1.0216 Precisión 0.4669\n",
      "Epoch 1 Lote 2250 Pérdida 1.0182 Precisión 0.4673\n",
      "Epoch 1 Lote 2300 Pérdida 1.0151 Precisión 0.4676\n",
      "Epoch 1 Lote 2350 Pérdida 1.0122 Precisión 0.4679\n",
      "Epoch 1 Lote 2400 Pérdida 1.0086 Precisión 0.4684\n",
      "Epoch 1 Lote 2450 Pérdida 1.0047 Precisión 0.4688\n",
      "Epoch 1 Lote 2500 Pérdida 1.0012 Precisión 0.4691\n",
      "Epoch 1 Lote 2550 Pérdida 0.9973 Precisión 0.4694\n",
      "Epoch 1 Lote 2600 Pérdida 0.9941 Precisión 0.4698\n",
      "Epoch 1 Lote 2650 Pérdida 0.9914 Precisión 0.4701\n",
      "Epoch 1 Lote 2700 Pérdida 0.9885 Precisión 0.4704\n",
      "Epoch 1 Lote 2750 Pérdida 0.9863 Precisión 0.4707\n",
      "Epoch 1 Lote 2800 Pérdida 0.9837 Precisión 0.4709\n",
      "Epoch 1 Lote 2850 Pérdida 0.9814 Precisión 0.4713\n",
      "Epoch 1 Lote 2900 Pérdida 0.9796 Precisión 0.4717\n",
      "Epoch 1 Lote 2950 Pérdida 0.9777 Precisión 0.4720\n",
      "Epoch 1 Lote 3000 Pérdida 0.9758 Precisión 0.4723\n",
      "Epoch 1 Lote 3050 Pérdida 0.9740 Precisión 0.4727\n",
      "Epoch 1 Lote 3100 Pérdida 0.9724 Precisión 0.4730\n",
      "Epoch 1 Lote 3150 Pérdida 0.9707 Precisión 0.4734\n",
      "Epoch 1 Lote 3200 Pérdida 0.9691 Precisión 0.4738\n",
      "Epoch 1 Lote 3250 Pérdida 0.9678 Precisión 0.4741\n",
      "Epoch 1 Lote 3300 Pérdida 0.9664 Precisión 0.4744\n",
      "Epoch 1 Lote 3350 Pérdida 0.9654 Precisión 0.4747\n",
      "Epoch 1 Lote 3400 Pérdida 0.9641 Precisión 0.4750\n",
      "Epoch 1 Lote 3450 Pérdida 0.9626 Precisión 0.4755\n",
      "Epoch 1 Lote 3500 Pérdida 0.9611 Precisión 0.4758\n",
      "Epoch 1 Lote 3550 Pérdida 0.9594 Precisión 0.4761\n",
      "Epoch 1 Lote 3600 Pérdida 0.9583 Precisión 0.4765\n",
      "Epoch 1 Lote 3650 Pérdida 0.9574 Precisión 0.4768\n",
      "Epoch 1 Lote 3700 Pérdida 0.9561 Precisión 0.4771\n",
      "Epoch 1 Lote 3750 Pérdida 0.9548 Precisión 0.4775\n",
      "Epoch 1 Lote 3800 Pérdida 0.9535 Precisión 0.4778\n",
      "Epoch 1 Lote 3850 Pérdida 0.9523 Precisión 0.4781\n",
      "Epoch 1 Lote 3900 Pérdida 0.9507 Precisión 0.4784\n",
      "Epoch 1 Lote 3950 Pérdida 0.9494 Precisión 0.4788\n",
      "Epoch 1 Lote 4000 Pérdida 0.9481 Precisión 0.4790\n",
      "Epoch 1 Lote 4050 Pérdida 0.9467 Precisión 0.4794\n",
      "Epoch 1 Lote 4100 Pérdida 0.9452 Precisión 0.4797\n",
      "Epoch 1 Lote 4150 Pérdida 0.9437 Precisión 0.4801\n",
      "Epoch 1 Lote 4200 Pérdida 0.9421 Precisión 0.4805\n",
      "Epoch 1 Lote 4250 Pérdida 0.9405 Precisión 0.4808\n",
      "Epoch 1 Lote 4300 Pérdida 0.9393 Precisión 0.4812\n",
      "Epoch 1 Lote 4350 Pérdida 0.9381 Precisión 0.4816\n",
      "Epoch 1 Lote 4400 Pérdida 0.9367 Precisión 0.4820\n",
      "Epoch 1 Lote 4450 Pérdida 0.9351 Precisión 0.4824\n",
      "Epoch 1 Lote 4500 Pérdida 0.9338 Precisión 0.4829\n",
      "Epoch 1 Lote 4550 Pérdida 0.9327 Precisión 0.4833\n",
      "Epoch 1 Lote 4600 Pérdida 0.9314 Precisión 0.4836\n",
      "Epoch 1 Lote 4650 Pérdida 0.9304 Precisión 0.4839\n",
      "Epoch 1 Lote 4700 Pérdida 0.9300 Precisión 0.4841\n",
      "Epoch 1 Lote 4750 Pérdida 0.9305 Precisión 0.4842\n",
      "Epoch 1 Lote 4800 Pérdida 0.9315 Precisión 0.4842\n",
      "Epoch 1 Lote 4850 Pérdida 0.9324 Precisión 0.4842\n",
      "Epoch 1 Lote 4900 Pérdida 0.9336 Precisión 0.4841\n",
      "Epoch 1 Lote 4950 Pérdida 0.9352 Precisión 0.4839\n",
      "Epoch 1 Lote 5000 Pérdida 0.9368 Precisión 0.4837\n",
      "Epoch 1 Lote 5050 Pérdida 0.9386 Precisión 0.4834\n",
      "Epoch 1 Lote 5100 Pérdida 0.9406 Precisión 0.4832\n",
      "Epoch 1 Lote 5150 Pérdida 0.9428 Precisión 0.4830\n",
      "Epoch 1 Lote 5200 Pérdida 0.9449 Precisión 0.4827\n",
      "Epoch 1 Lote 5250 Pérdida 0.9469 Precisión 0.4824\n",
      "Epoch 1 Lote 5300 Pérdida 0.9489 Precisión 0.4822\n",
      "Epoch 1 Lote 5350 Pérdida 0.9514 Precisión 0.4819\n",
      "Epoch 1 Lote 5400 Pérdida 0.9534 Precisión 0.4816\n",
      "Epoch 1 Lote 5450 Pérdida 0.9554 Precisión 0.4813\n",
      "Epoch 1 Lote 5500 Pérdida 0.9574 Precisión 0.4811\n",
      "Epoch 1 Lote 5550 Pérdida 0.9592 Precisión 0.4808\n",
      "Epoch 1 Lote 5600 Pérdida 0.9612 Precisión 0.4806\n",
      "Epoch 1 Lote 5650 Pérdida 0.9631 Precisión 0.4803\n",
      "Epoch 1 Lote 5700 Pérdida 0.9651 Precisión 0.4801\n",
      "Epoch 1 Lote 5750 Pérdida 0.9672 Precisión 0.4798\n",
      "Epoch 1 Lote 5800 Pérdida 0.9692 Precisión 0.4794\n",
      "Epoch 1 Lote 5850 Pérdida 0.9712 Precisión 0.4791\n",
      "Epoch 1 Lote 5900 Pérdida 0.9728 Precisión 0.4788\n",
      "Epoch 1 Lote 5950 Pérdida 0.9747 Precisión 0.4785\n",
      "Epoch 1 Lote 6000 Pérdida 0.9765 Precisión 0.4781\n",
      "Epoch 1 Lote 6050 Pérdida 0.9782 Precisión 0.4778\n",
      "Epoch 1 Lote 6100 Pérdida 0.9799 Precisión 0.4774\n",
      "Epoch 1 Lote 6150 Pérdida 0.9815 Precisión 0.4771\n",
      "Epoch 1 Lote 6200 Pérdida 0.9830 Precisión 0.4768\n",
      "Epoch 1 Lote 6250 Pérdida 0.9844 Precisión 0.4765\n",
      "Epoch 1 Lote 6300 Pérdida 0.9858 Precisión 0.4763\n",
      "Epoch 1 Lote 6350 Pérdida 0.9870 Precisión 0.4760\n",
      "Epoch 1 Lote 6400 Pérdida 0.9884 Precisión 0.4757\n",
      "Guardando checkpoint para el epoch 1 en ckpt/ckpt-6\n",
      "Tiempo que ha tardado 1 epoch: 4040.3445274829865 segs\n",
      "\n",
      "Inicio del epoch 2\n",
      "Epoch 2 Lote 0 Pérdida 1.2224 Precisión 0.4490\n",
      "Epoch 2 Lote 50 Pérdida 1.1621 Precisión 0.4490\n",
      "Epoch 2 Lote 100 Pérdida 1.1601 Precisión 0.4468\n",
      "Epoch 2 Lote 150 Pérdida 1.1619 Precisión 0.4478\n",
      "Epoch 2 Lote 200 Pérdida 1.1642 Precisión 0.4476\n",
      "Epoch 2 Lote 250 Pérdida 1.1638 Precisión 0.4481\n",
      "Epoch 2 Lote 300 Pérdida 1.1596 Precisión 0.4484\n",
      "Epoch 2 Lote 350 Pérdida 1.1590 Precisión 0.4487\n",
      "Epoch 2 Lote 400 Pérdida 1.1585 Precisión 0.4489\n",
      "Epoch 2 Lote 450 Pérdida 1.1558 Precisión 0.4488\n",
      "Epoch 2 Lote 500 Pérdida 1.1522 Precisión 0.4490\n",
      "Epoch 2 Lote 550 Pérdida 1.1467 Precisión 0.4489\n",
      "Epoch 2 Lote 600 Pérdida 1.1441 Precisión 0.4490\n",
      "Epoch 2 Lote 650 Pérdida 1.1429 Precisión 0.4489\n",
      "Epoch 2 Lote 700 Pérdida 1.1416 Precisión 0.4492\n",
      "Epoch 2 Lote 750 Pérdida 1.1389 Precisión 0.4498\n",
      "Epoch 2 Lote 800 Pérdida 1.1342 Precisión 0.4508\n",
      "Epoch 2 Lote 850 Pérdida 1.1282 Precisión 0.4519\n",
      "Epoch 2 Lote 900 Pérdida 1.1228 Precisión 0.4529\n",
      "Epoch 2 Lote 950 Pérdida 1.1157 Precisión 0.4542\n",
      "Epoch 2 Lote 1000 Pérdida 1.1091 Precisión 0.4553\n",
      "Epoch 2 Lote 1050 Pérdida 1.1031 Precisión 0.4563\n",
      "Epoch 2 Lote 1100 Pérdida 1.0968 Precisión 0.4570\n",
      "Epoch 2 Lote 1150 Pérdida 1.0906 Precisión 0.4577\n",
      "Epoch 2 Lote 1200 Pérdida 1.0857 Precisión 0.4585\n",
      "Epoch 2 Lote 1250 Pérdida 1.0793 Precisión 0.4595\n",
      "Epoch 2 Lote 1300 Pérdida 1.0736 Precisión 0.4602\n",
      "Epoch 2 Lote 1350 Pérdida 1.0675 Precisión 0.4611\n",
      "Epoch 2 Lote 1400 Pérdida 1.0609 Precisión 0.4618\n",
      "Epoch 2 Lote 1450 Pérdida 1.0558 Precisión 0.4626\n",
      "Epoch 2 Lote 1500 Pérdida 1.0513 Precisión 0.4633\n",
      "Epoch 2 Lote 1550 Pérdida 1.0472 Precisión 0.4640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Lote 1600 Pérdida 1.0420 Precisión 0.4648\n",
      "Epoch 2 Lote 1650 Pérdida 1.0375 Precisión 0.4655\n",
      "Epoch 2 Lote 1700 Pérdida 1.0328 Precisión 0.4663\n",
      "Epoch 2 Lote 1750 Pérdida 1.0285 Precisión 0.4670\n",
      "Epoch 2 Lote 1800 Pérdida 1.0246 Precisión 0.4676\n",
      "Epoch 2 Lote 1850 Pérdida 1.0204 Precisión 0.4681\n",
      "Epoch 2 Lote 1900 Pérdida 1.0161 Precisión 0.4686\n",
      "Epoch 2 Lote 1950 Pérdida 1.0120 Precisión 0.4689\n",
      "Epoch 2 Lote 2000 Pérdida 1.0082 Precisión 0.4692\n",
      "Epoch 2 Lote 2050 Pérdida 1.0042 Precisión 0.4696\n",
      "Epoch 2 Lote 2100 Pérdida 1.0007 Precisión 0.4700\n",
      "Epoch 2 Lote 2150 Pérdida 0.9968 Precisión 0.4705\n",
      "Epoch 2 Lote 2200 Pérdida 0.9934 Precisión 0.4708\n",
      "Epoch 2 Lote 2250 Pérdida 0.9903 Precisión 0.4715\n",
      "Epoch 2 Lote 2300 Pérdida 0.9870 Precisión 0.4719\n",
      "Epoch 2 Lote 2350 Pérdida 0.9835 Precisión 0.4721\n",
      "Epoch 2 Lote 2400 Pérdida 0.9802 Precisión 0.4725\n",
      "Epoch 2 Lote 2450 Pérdida 0.9769 Precisión 0.4728\n",
      "Epoch 2 Lote 2500 Pérdida 0.9733 Precisión 0.4732\n",
      "Epoch 2 Lote 2550 Pérdida 0.9699 Precisión 0.4736\n",
      "Epoch 2 Lote 2600 Pérdida 0.9667 Precisión 0.4739\n",
      "Epoch 2 Lote 2650 Pérdida 0.9640 Precisión 0.4743\n",
      "Epoch 2 Lote 2700 Pérdida 0.9618 Precisión 0.4745\n",
      "Epoch 2 Lote 2750 Pérdida 0.9593 Precisión 0.4749\n",
      "Epoch 2 Lote 2800 Pérdida 0.9567 Precisión 0.4751\n",
      "Epoch 2 Lote 2850 Pérdida 0.9549 Precisión 0.4755\n",
      "Epoch 2 Lote 2900 Pérdida 0.9528 Precisión 0.4758\n",
      "Epoch 2 Lote 2950 Pérdida 0.9511 Precisión 0.4762\n",
      "Epoch 2 Lote 3000 Pérdida 0.9491 Precisión 0.4766\n",
      "Epoch 2 Lote 3050 Pérdida 0.9474 Precisión 0.4769\n",
      "Epoch 2 Lote 3100 Pérdida 0.9458 Precisión 0.4772\n",
      "Epoch 2 Lote 3150 Pérdida 0.9443 Precisión 0.4775\n",
      "Epoch 2 Lote 3200 Pérdida 0.9431 Precisión 0.4778\n",
      "Epoch 2 Lote 3250 Pérdida 0.9417 Precisión 0.4782\n",
      "Epoch 2 Lote 3300 Pérdida 0.9407 Precisión 0.4785\n",
      "Epoch 2 Lote 3350 Pérdida 0.9392 Precisión 0.4789\n",
      "Epoch 2 Lote 3400 Pérdida 0.9378 Precisión 0.4792\n",
      "Epoch 2 Lote 3450 Pérdida 0.9366 Precisión 0.4796\n",
      "Epoch 2 Lote 3500 Pérdida 0.9349 Precisión 0.4799\n",
      "Epoch 2 Lote 3550 Pérdida 0.9337 Precisión 0.4802\n",
      "Epoch 2 Lote 3600 Pérdida 0.9324 Precisión 0.4805\n",
      "Epoch 2 Lote 3650 Pérdida 0.9312 Precisión 0.4809\n",
      "Epoch 2 Lote 3700 Pérdida 0.9300 Precisión 0.4812\n",
      "Epoch 2 Lote 3750 Pérdida 0.9286 Precisión 0.4815\n",
      "Epoch 2 Lote 3800 Pérdida 0.9271 Precisión 0.4819\n",
      "Epoch 2 Lote 3850 Pérdida 0.9256 Precisión 0.4821\n",
      "Epoch 2 Lote 3900 Pérdida 0.9245 Precisión 0.4825\n",
      "Epoch 2 Lote 3950 Pérdida 0.9231 Precisión 0.4828\n",
      "Epoch 2 Lote 4000 Pérdida 0.9218 Precisión 0.4831\n",
      "Epoch 2 Lote 4050 Pérdida 0.9207 Precisión 0.4835\n",
      "Epoch 2 Lote 4100 Pérdida 0.9189 Precisión 0.4839\n",
      "Epoch 2 Lote 4150 Pérdida 0.9179 Precisión 0.4842\n",
      "Epoch 2 Lote 4200 Pérdida 0.9163 Precisión 0.4846\n",
      "Epoch 2 Lote 4250 Pérdida 0.9149 Precisión 0.4850\n",
      "Epoch 2 Lote 4300 Pérdida 0.9136 Precisión 0.4855\n",
      "Epoch 2 Lote 4350 Pérdida 0.9123 Precisión 0.4859\n",
      "Epoch 2 Lote 4400 Pérdida 0.9109 Precisión 0.4863\n",
      "Epoch 2 Lote 4450 Pérdida 0.9096 Precisión 0.4866\n",
      "Epoch 2 Lote 4500 Pérdida 0.9084 Precisión 0.4871\n",
      "Epoch 2 Lote 4550 Pérdida 0.9070 Precisión 0.4874\n",
      "Epoch 2 Lote 4600 Pérdida 0.9058 Precisión 0.4878\n",
      "Epoch 2 Lote 4650 Pérdida 0.9047 Precisión 0.4881\n",
      "Epoch 2 Lote 4700 Pérdida 0.9044 Precisión 0.4883\n",
      "Epoch 2 Lote 4750 Pérdida 0.9048 Precisión 0.4885\n",
      "Epoch 2 Lote 4800 Pérdida 0.9057 Precisión 0.4885\n",
      "Epoch 2 Lote 4850 Pérdida 0.9067 Precisión 0.4884\n",
      "Epoch 2 Lote 4900 Pérdida 0.9079 Precisión 0.4883\n",
      "Epoch 2 Lote 4950 Pérdida 0.9095 Precisión 0.4881\n",
      "Epoch 2 Lote 5000 Pérdida 0.9113 Precisión 0.4879\n",
      "Epoch 2 Lote 5050 Pérdida 0.9132 Precisión 0.4876\n",
      "Epoch 2 Lote 5100 Pérdida 0.9154 Precisión 0.4874\n",
      "Epoch 2 Lote 5150 Pérdida 0.9174 Precisión 0.4870\n",
      "Epoch 2 Lote 5200 Pérdida 0.9195 Precisión 0.4867\n",
      "Epoch 2 Lote 5250 Pérdida 0.9218 Precisión 0.4865\n",
      "Epoch 2 Lote 5300 Pérdida 0.9240 Precisión 0.4862\n",
      "Epoch 2 Lote 5350 Pérdida 0.9262 Precisión 0.4859\n",
      "Epoch 2 Lote 5400 Pérdida 0.9282 Precisión 0.4857\n",
      "Epoch 2 Lote 5450 Pérdida 0.9301 Precisión 0.4854\n",
      "Epoch 2 Lote 5500 Pérdida 0.9321 Precisión 0.4851\n",
      "Epoch 2 Lote 5550 Pérdida 0.9344 Precisión 0.4849\n",
      "Epoch 2 Lote 5600 Pérdida 0.9361 Precisión 0.4846\n",
      "Epoch 2 Lote 5650 Pérdida 0.9380 Precisión 0.4843\n",
      "Epoch 2 Lote 5700 Pérdida 0.9399 Precisión 0.4840\n",
      "Epoch 2 Lote 5750 Pérdida 0.9419 Precisión 0.4837\n",
      "Epoch 2 Lote 5800 Pérdida 0.9441 Precisión 0.4834\n",
      "Epoch 2 Lote 5850 Pérdida 0.9459 Precisión 0.4831\n",
      "Epoch 2 Lote 5900 Pérdida 0.9478 Precisión 0.4828\n",
      "Epoch 2 Lote 5950 Pérdida 0.9498 Precisión 0.4825\n",
      "Epoch 2 Lote 6000 Pérdida 0.9516 Precisión 0.4822\n",
      "Epoch 2 Lote 6050 Pérdida 0.9530 Precisión 0.4819\n",
      "Epoch 2 Lote 6100 Pérdida 0.9549 Precisión 0.4816\n",
      "Epoch 2 Lote 6150 Pérdida 0.9565 Precisión 0.4812\n",
      "Epoch 2 Lote 6200 Pérdida 0.9580 Precisión 0.4809\n",
      "Epoch 2 Lote 6250 Pérdida 0.9595 Precisión 0.4806\n",
      "Epoch 2 Lote 6300 Pérdida 0.9610 Precisión 0.4803\n",
      "Epoch 2 Lote 6350 Pérdida 0.9626 Precisión 0.4801\n",
      "Epoch 2 Lote 6400 Pérdida 0.9638 Precisión 0.4798\n",
      "Guardando checkpoint para el epoch 2 en ckpt/ckpt-7\n",
      "Tiempo que ha tardado 1 epoch: 3679.1943593025208 segs\n",
      "\n",
      "Inicio del epoch 3\n",
      "Epoch 3 Lote 0 Pérdida 1.1475 Precisión 0.4581\n",
      "Epoch 3 Lote 50 Pérdida 1.1469 Precisión 0.4502\n",
      "Epoch 3 Lote 100 Pérdida 1.1394 Precisión 0.4492\n",
      "Epoch 3 Lote 150 Pérdida 1.1324 Precisión 0.4515\n",
      "Epoch 3 Lote 200 Pérdida 1.1310 Precisión 0.4518\n",
      "Epoch 3 Lote 250 Pérdida 1.1258 Precisión 0.4521\n",
      "Epoch 3 Lote 300 Pérdida 1.1261 Precisión 0.4527\n",
      "Epoch 3 Lote 350 Pérdida 1.1236 Precisión 0.4527\n",
      "Epoch 3 Lote 400 Pérdida 1.1221 Precisión 0.4522\n",
      "Epoch 3 Lote 450 Pérdida 1.1231 Precisión 0.4520\n",
      "Epoch 3 Lote 500 Pérdida 1.1213 Precisión 0.4516\n",
      "Epoch 3 Lote 550 Pérdida 1.1191 Precisión 0.4518\n",
      "Epoch 3 Lote 600 Pérdida 1.1158 Precisión 0.4521\n",
      "Epoch 3 Lote 650 Pérdida 1.1166 Precisión 0.4522\n",
      "Epoch 3 Lote 700 Pérdida 1.1158 Precisión 0.4527\n",
      "Epoch 3 Lote 750 Pérdida 1.1130 Precisión 0.4536\n",
      "Epoch 3 Lote 800 Pérdida 1.1092 Precisión 0.4544\n",
      "Epoch 3 Lote 850 Pérdida 1.1050 Precisión 0.4556\n",
      "Epoch 3 Lote 900 Pérdida 1.0993 Precisión 0.4567\n",
      "Epoch 3 Lote 950 Pérdida 1.0932 Precisión 0.4577\n",
      "Epoch 3 Lote 1000 Pérdida 1.0876 Precisión 0.4587\n",
      "Epoch 3 Lote 1050 Pérdida 1.0811 Precisión 0.4597\n",
      "Epoch 3 Lote 1100 Pérdida 1.0746 Precisión 0.4606\n",
      "Epoch 3 Lote 1150 Pérdida 1.0675 Precisión 0.4615\n",
      "Epoch 3 Lote 1200 Pérdida 1.0614 Precisión 0.4622\n",
      "Epoch 3 Lote 1250 Pérdida 1.0557 Precisión 0.4630\n",
      "Epoch 3 Lote 1300 Pérdida 1.0505 Precisión 0.4637\n",
      "Epoch 3 Lote 1350 Pérdida 1.0452 Precisión 0.4645\n",
      "Epoch 3 Lote 1400 Pérdida 1.0395 Precisión 0.4653\n",
      "Epoch 3 Lote 1450 Pérdida 1.0348 Precisión 0.4662\n",
      "Epoch 3 Lote 1500 Pérdida 1.0299 Precisión 0.4669\n",
      "Epoch 3 Lote 1550 Pérdida 1.0252 Precisión 0.4675\n",
      "Epoch 3 Lote 1600 Pérdida 1.0205 Precisión 0.4684\n",
      "Epoch 3 Lote 1650 Pérdida 1.0157 Precisión 0.4692\n",
      "Epoch 3 Lote 1700 Pérdida 1.0105 Precisión 0.4699\n",
      "Epoch 3 Lote 1750 Pérdida 1.0061 Precisión 0.4705\n",
      "Epoch 3 Lote 1800 Pérdida 1.0022 Precisión 0.4712\n",
      "Epoch 3 Lote 1850 Pérdida 0.9984 Precisión 0.4717\n",
      "Epoch 3 Lote 1900 Pérdida 0.9945 Precisión 0.4722\n",
      "Epoch 3 Lote 1950 Pérdida 0.9903 Precisión 0.4726\n",
      "Epoch 3 Lote 2000 Pérdida 0.9865 Precisión 0.4730\n",
      "Epoch 3 Lote 2050 Pérdida 0.9829 Precisión 0.4732\n",
      "Epoch 3 Lote 2100 Pérdida 0.9793 Precisión 0.4736\n",
      "Epoch 3 Lote 2150 Pérdida 0.9755 Precisión 0.4742\n",
      "Epoch 3 Lote 2200 Pérdida 0.9724 Precisión 0.4746\n",
      "Epoch 3 Lote 2250 Pérdida 0.9694 Precisión 0.4749\n",
      "Epoch 3 Lote 2300 Pérdida 0.9658 Precisión 0.4752\n",
      "Epoch 3 Lote 2350 Pérdida 0.9624 Precisión 0.4756\n",
      "Epoch 3 Lote 2400 Pérdida 0.9589 Precisión 0.4761\n",
      "Epoch 3 Lote 2450 Pérdida 0.9559 Precisión 0.4764\n",
      "Epoch 3 Lote 2500 Pérdida 0.9520 Precisión 0.4767\n",
      "Epoch 3 Lote 2550 Pérdida 0.9485 Precisión 0.4770\n",
      "Epoch 3 Lote 2600 Pérdida 0.9454 Precisión 0.4773\n",
      "Epoch 3 Lote 2650 Pérdida 0.9429 Precisión 0.4776\n",
      "Epoch 3 Lote 2700 Pérdida 0.9403 Precisión 0.4779\n",
      "Epoch 3 Lote 2750 Pérdida 0.9383 Precisión 0.4783\n",
      "Epoch 3 Lote 2800 Pérdida 0.9361 Precisión 0.4786\n",
      "Epoch 3 Lote 2850 Pérdida 0.9341 Precisión 0.4789\n",
      "Epoch 3 Lote 2900 Pérdida 0.9320 Precisión 0.4792\n",
      "Epoch 3 Lote 2950 Pérdida 0.9299 Precisión 0.4796\n",
      "Epoch 3 Lote 3000 Pérdida 0.9282 Precisión 0.4799\n",
      "Epoch 3 Lote 3050 Pérdida 0.9270 Precisión 0.4803\n",
      "Epoch 3 Lote 3100 Pérdida 0.9254 Precisión 0.4806\n",
      "Epoch 3 Lote 3150 Pérdida 0.9236 Precisión 0.4810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Lote 3200 Pérdida 0.9220 Precisión 0.4814\n",
      "Epoch 3 Lote 3250 Pérdida 0.9208 Precisión 0.4817\n",
      "Epoch 3 Lote 3300 Pérdida 0.9196 Precisión 0.4820\n",
      "Epoch 3 Lote 3350 Pérdida 0.9180 Precisión 0.4822\n",
      "Epoch 3 Lote 3400 Pérdida 0.9168 Precisión 0.4826\n",
      "Epoch 3 Lote 3450 Pérdida 0.9161 Precisión 0.4829\n",
      "Epoch 3 Lote 3500 Pérdida 0.9147 Precisión 0.4831\n",
      "Epoch 3 Lote 3550 Pérdida 0.9135 Precisión 0.4835\n",
      "Epoch 3 Lote 3600 Pérdida 0.9121 Precisión 0.4838\n",
      "Epoch 3 Lote 3650 Pérdida 0.9103 Precisión 0.4841\n",
      "Epoch 3 Lote 3700 Pérdida 0.9090 Precisión 0.4845\n",
      "Epoch 3 Lote 3750 Pérdida 0.9078 Precisión 0.4848\n",
      "Epoch 3 Lote 3800 Pérdida 0.9066 Precisión 0.4852\n",
      "Epoch 3 Lote 3850 Pérdida 0.9053 Precisión 0.4855\n",
      "Epoch 3 Lote 3900 Pérdida 0.9040 Precisión 0.4858\n",
      "Epoch 3 Lote 3950 Pérdida 0.9028 Precisión 0.4860\n",
      "Epoch 3 Lote 4000 Pérdida 0.9015 Precisión 0.4863\n",
      "Epoch 3 Lote 4050 Pérdida 0.9003 Precisión 0.4867\n",
      "Epoch 3 Lote 4100 Pérdida 0.8988 Precisión 0.4870\n",
      "Epoch 3 Lote 4150 Pérdida 0.8972 Precisión 0.4874\n",
      "Epoch 3 Lote 4200 Pérdida 0.8959 Precisión 0.4878\n",
      "Epoch 3 Lote 4250 Pérdida 0.8944 Precisión 0.4881\n",
      "Epoch 3 Lote 4300 Pérdida 0.8932 Precisión 0.4886\n",
      "Epoch 3 Lote 4350 Pérdida 0.8917 Precisión 0.4891\n",
      "Epoch 3 Lote 4400 Pérdida 0.8906 Precisión 0.4895\n",
      "Epoch 3 Lote 4450 Pérdida 0.8895 Precisión 0.4898\n",
      "Epoch 3 Lote 4500 Pérdida 0.8882 Precisión 0.4902\n",
      "Epoch 3 Lote 4550 Pérdida 0.8871 Precisión 0.4907\n",
      "Epoch 3 Lote 4600 Pérdida 0.8859 Precisión 0.4911\n",
      "Epoch 3 Lote 4650 Pérdida 0.8849 Precisión 0.4914\n",
      "Epoch 3 Lote 4700 Pérdida 0.8848 Precisión 0.4916\n",
      "Epoch 3 Lote 4750 Pérdida 0.8851 Precisión 0.4917\n",
      "Epoch 3 Lote 4800 Pérdida 0.8856 Precisión 0.4917\n",
      "Epoch 3 Lote 4850 Pérdida 0.8864 Precisión 0.4916\n",
      "Epoch 3 Lote 4900 Pérdida 0.8878 Precisión 0.4914\n",
      "Epoch 3 Lote 4950 Pérdida 0.8895 Precisión 0.4912\n",
      "Epoch 3 Lote 5000 Pérdida 0.8913 Precisión 0.4910\n",
      "Epoch 3 Lote 5050 Pérdida 0.8932 Precisión 0.4908\n",
      "Epoch 3 Lote 5100 Pérdida 0.8951 Precisión 0.4906\n",
      "Epoch 3 Lote 5150 Pérdida 0.8973 Precisión 0.4903\n",
      "Epoch 3 Lote 5200 Pérdida 0.8994 Precisión 0.4900\n",
      "Epoch 3 Lote 5250 Pérdida 0.9018 Precisión 0.4898\n",
      "Epoch 3 Lote 5300 Pérdida 0.9040 Precisión 0.4895\n",
      "Epoch 3 Lote 5350 Pérdida 0.9059 Precisión 0.4891\n",
      "Epoch 3 Lote 5400 Pérdida 0.9079 Precisión 0.4889\n",
      "Epoch 3 Lote 5450 Pérdida 0.9101 Precisión 0.4886\n",
      "Epoch 3 Lote 5500 Pérdida 0.9120 Precisión 0.4883\n",
      "Epoch 3 Lote 5550 Pérdida 0.9141 Precisión 0.4881\n",
      "Epoch 3 Lote 5600 Pérdida 0.9159 Precisión 0.4877\n",
      "Epoch 3 Lote 5650 Pérdida 0.9179 Precisión 0.4875\n",
      "Epoch 3 Lote 5700 Pérdida 0.9198 Precisión 0.4872\n",
      "Epoch 3 Lote 5750 Pérdida 0.9219 Precisión 0.4869\n",
      "Epoch 3 Lote 5800 Pérdida 0.9238 Precisión 0.4866\n",
      "Epoch 3 Lote 5850 Pérdida 0.9257 Precisión 0.4863\n",
      "Epoch 3 Lote 5900 Pérdida 0.9279 Precisión 0.4860\n",
      "Epoch 3 Lote 5950 Pérdida 0.9297 Precisión 0.4856\n",
      "Epoch 3 Lote 6000 Pérdida 0.9316 Precisión 0.4852\n",
      "Epoch 3 Lote 6050 Pérdida 0.9333 Precisión 0.4849\n",
      "Epoch 3 Lote 6100 Pérdida 0.9350 Precisión 0.4846\n",
      "Epoch 3 Lote 6150 Pérdida 0.9366 Precisión 0.4843\n",
      "Epoch 3 Lote 6200 Pérdida 0.9383 Precisión 0.4841\n",
      "Epoch 3 Lote 6250 Pérdida 0.9400 Precisión 0.4837\n",
      "Epoch 3 Lote 6300 Pérdida 0.9414 Precisión 0.4834\n",
      "Epoch 3 Lote 6350 Pérdida 0.9429 Precisión 0.4831\n",
      "Epoch 3 Lote 6400 Pérdida 0.9441 Precisión 0.4829\n",
      "Guardando checkpoint para el epoch 3 en ckpt/ckpt-8\n",
      "Tiempo que ha tardado 1 epoch: 3638.7801597118378 segs\n",
      "\n",
      "Inicio del epoch 4\n",
      "Epoch 4 Lote 0 Pérdida 1.1471 Precisión 0.4301\n",
      "Epoch 4 Lote 50 Pérdida 1.1107 Precisión 0.4503\n",
      "Epoch 4 Lote 100 Pérdida 1.1151 Precisión 0.4517\n",
      "Epoch 4 Lote 150 Pérdida 1.1102 Precisión 0.4536\n",
      "Epoch 4 Lote 200 Pérdida 1.1164 Precisión 0.4547\n",
      "Epoch 4 Lote 250 Pérdida 1.1156 Precisión 0.4542\n",
      "Epoch 4 Lote 300 Pérdida 1.1117 Precisión 0.4544\n",
      "Epoch 4 Lote 350 Pérdida 1.1137 Precisión 0.4543\n",
      "Epoch 4 Lote 400 Pérdida 1.1146 Precisión 0.4554\n",
      "Epoch 4 Lote 450 Pérdida 1.1137 Precisión 0.4552\n",
      "Epoch 4 Lote 500 Pérdida 1.1100 Precisión 0.4552\n",
      "Epoch 4 Lote 550 Pérdida 1.1083 Precisión 0.4550\n",
      "Epoch 4 Lote 600 Pérdida 1.1069 Precisión 0.4549\n",
      "Epoch 4 Lote 650 Pérdida 1.1044 Precisión 0.4551\n",
      "Epoch 4 Lote 700 Pérdida 1.1020 Precisión 0.4553\n",
      "Epoch 4 Lote 750 Pérdida 1.0990 Precisión 0.4559\n",
      "Epoch 4 Lote 800 Pérdida 1.0948 Precisión 0.4571\n",
      "Epoch 4 Lote 850 Pérdida 1.0890 Precisión 0.4580\n",
      "Epoch 4 Lote 900 Pérdida 1.0830 Precisión 0.4593\n",
      "Epoch 4 Lote 950 Pérdida 1.0760 Precisión 0.4604\n",
      "Epoch 4 Lote 1000 Pérdida 1.0707 Precisión 0.4617\n",
      "Epoch 4 Lote 1050 Pérdida 1.0636 Precisión 0.4627\n",
      "Epoch 4 Lote 1100 Pérdida 1.0573 Precisión 0.4637\n",
      "Epoch 4 Lote 1150 Pérdida 1.0516 Precisión 0.4643\n",
      "Epoch 4 Lote 1200 Pérdida 1.0463 Precisión 0.4652\n",
      "Epoch 4 Lote 1250 Pérdida 1.0401 Precisión 0.4659\n",
      "Epoch 4 Lote 1300 Pérdida 1.0338 Precisión 0.4667\n",
      "Epoch 4 Lote 1350 Pérdida 1.0290 Precisión 0.4675\n",
      "Epoch 4 Lote 1400 Pérdida 1.0234 Precisión 0.4682\n",
      "Epoch 4 Lote 1450 Pérdida 1.0181 Precisión 0.4690\n",
      "Epoch 4 Lote 1500 Pérdida 1.0130 Precisión 0.4697\n",
      "Epoch 4 Lote 1550 Pérdida 1.0081 Precisión 0.4703\n",
      "Epoch 4 Lote 1600 Pérdida 1.0036 Precisión 0.4710\n",
      "Epoch 4 Lote 1650 Pérdida 0.9991 Precisión 0.4718\n",
      "Epoch 4 Lote 1700 Pérdida 0.9948 Precisión 0.4725\n",
      "Epoch 4 Lote 1750 Pérdida 0.9899 Precisión 0.4733\n",
      "Epoch 4 Lote 1800 Pérdida 0.9856 Precisión 0.4739\n",
      "Epoch 4 Lote 1850 Pérdida 0.9811 Precisión 0.4743\n",
      "Epoch 4 Lote 1900 Pérdida 0.9772 Precisión 0.4747\n",
      "Epoch 4 Lote 1950 Pérdida 0.9734 Precisión 0.4751\n",
      "Epoch 4 Lote 2000 Pérdida 0.9693 Precisión 0.4754\n",
      "Epoch 4 Lote 2050 Pérdida 0.9654 Precisión 0.4759\n",
      "Epoch 4 Lote 2100 Pérdida 0.9618 Precisión 0.4764\n",
      "Epoch 4 Lote 2150 Pérdida 0.9583 Precisión 0.4767\n",
      "Epoch 4 Lote 2200 Pérdida 0.9552 Precisión 0.4771\n",
      "Epoch 4 Lote 2250 Pérdida 0.9521 Precisión 0.4775\n",
      "Epoch 4 Lote 2300 Pérdida 0.9491 Precisión 0.4779\n",
      "Epoch 4 Lote 2350 Pérdida 0.9459 Precisión 0.4783\n",
      "Epoch 4 Lote 2400 Pérdida 0.9425 Precisión 0.4788\n",
      "Epoch 4 Lote 2450 Pérdida 0.9395 Precisión 0.4792\n",
      "Epoch 4 Lote 2500 Pérdida 0.9359 Precisión 0.4795\n",
      "Epoch 4 Lote 2550 Pérdida 0.9325 Precisión 0.4797\n",
      "Epoch 4 Lote 2600 Pérdida 0.9293 Precisión 0.4800\n",
      "Epoch 4 Lote 2650 Pérdida 0.9265 Precisión 0.4803\n",
      "Epoch 4 Lote 2700 Pérdida 0.9240 Precisión 0.4807\n",
      "Epoch 4 Lote 2750 Pérdida 0.9218 Precisión 0.4811\n",
      "Epoch 4 Lote 2800 Pérdida 0.9195 Precisión 0.4814\n",
      "Epoch 4 Lote 2850 Pérdida 0.9175 Precisión 0.4818\n",
      "Epoch 4 Lote 2900 Pérdida 0.9152 Precisión 0.4821\n",
      "Epoch 4 Lote 2950 Pérdida 0.9135 Precisión 0.4824\n",
      "Epoch 4 Lote 3000 Pérdida 0.9116 Precisión 0.4828\n",
      "Epoch 4 Lote 3050 Pérdida 0.9100 Precisión 0.4830\n",
      "Epoch 4 Lote 3100 Pérdida 0.9082 Precisión 0.4833\n",
      "Epoch 4 Lote 3150 Pérdida 0.9067 Precisión 0.4835\n",
      "Epoch 4 Lote 3200 Pérdida 0.9054 Precisión 0.4839\n",
      "Epoch 4 Lote 3250 Pérdida 0.9041 Precisión 0.4842\n",
      "Epoch 4 Lote 3300 Pérdida 0.9026 Precisión 0.4846\n",
      "Epoch 4 Lote 3350 Pérdida 0.9014 Precisión 0.4850\n",
      "Epoch 4 Lote 3400 Pérdida 0.9004 Precisión 0.4853\n",
      "Epoch 4 Lote 3450 Pérdida 0.8993 Precisión 0.4856\n",
      "Epoch 4 Lote 3500 Pérdida 0.8978 Precisión 0.4860\n",
      "Epoch 4 Lote 3550 Pérdida 0.8963 Precisión 0.4864\n",
      "Epoch 4 Lote 3600 Pérdida 0.8952 Precisión 0.4867\n",
      "Epoch 4 Lote 3650 Pérdida 0.8939 Precisión 0.4870\n",
      "Epoch 4 Lote 3700 Pérdida 0.8925 Precisión 0.4873\n",
      "Epoch 4 Lote 3750 Pérdida 0.8912 Precisión 0.4876\n",
      "Epoch 4 Lote 3800 Pérdida 0.8899 Precisión 0.4879\n",
      "Epoch 4 Lote 3850 Pérdida 0.8888 Precisión 0.4882\n",
      "Epoch 4 Lote 3900 Pérdida 0.8875 Precisión 0.4885\n",
      "Epoch 4 Lote 3950 Pérdida 0.8863 Precisión 0.4887\n",
      "Epoch 4 Lote 4000 Pérdida 0.8852 Precisión 0.4891\n",
      "Epoch 4 Lote 4050 Pérdida 0.8839 Precisión 0.4893\n",
      "Epoch 4 Lote 4100 Pérdida 0.8824 Precisión 0.4896\n",
      "Epoch 4 Lote 4150 Pérdida 0.8810 Precisión 0.4900\n",
      "Epoch 4 Lote 4200 Pérdida 0.8798 Precisión 0.4905\n",
      "Epoch 4 Lote 4250 Pérdida 0.8783 Precisión 0.4908\n",
      "Epoch 4 Lote 4300 Pérdida 0.8769 Precisión 0.4913\n",
      "Epoch 4 Lote 4350 Pérdida 0.8759 Precisión 0.4917\n",
      "Epoch 4 Lote 4400 Pérdida 0.8746 Precisión 0.4920\n",
      "Epoch 4 Lote 4450 Pérdida 0.8734 Precisión 0.4924\n",
      "Epoch 4 Lote 4500 Pérdida 0.8723 Precisión 0.4928\n",
      "Epoch 4 Lote 4550 Pérdida 0.8710 Precisión 0.4933\n",
      "Epoch 4 Lote 4600 Pérdida 0.8699 Precisión 0.4936\n",
      "Epoch 4 Lote 4650 Pérdida 0.8689 Precisión 0.4940\n",
      "Epoch 4 Lote 4700 Pérdida 0.8687 Precisión 0.4941\n",
      "Epoch 4 Lote 4750 Pérdida 0.8688 Precisión 0.4942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Lote 4800 Pérdida 0.8694 Precisión 0.4943\n",
      "Epoch 4 Lote 4850 Pérdida 0.8701 Precisión 0.4942\n",
      "Epoch 4 Lote 4900 Pérdida 0.8717 Precisión 0.4941\n",
      "Epoch 4 Lote 4950 Pérdida 0.8734 Precisión 0.4939\n",
      "Epoch 4 Lote 5000 Pérdida 0.8751 Precisión 0.4937\n",
      "Epoch 4 Lote 5050 Pérdida 0.8771 Precisión 0.4934\n",
      "Epoch 4 Lote 5100 Pérdida 0.8791 Precisión 0.4932\n",
      "Epoch 4 Lote 5150 Pérdida 0.8812 Precisión 0.4929\n",
      "Epoch 4 Lote 5200 Pérdida 0.8830 Precisión 0.4926\n",
      "Epoch 4 Lote 5250 Pérdida 0.8855 Precisión 0.4923\n",
      "Epoch 4 Lote 5300 Pérdida 0.8874 Precisión 0.4920\n",
      "Epoch 4 Lote 5350 Pérdida 0.8895 Precisión 0.4918\n",
      "Epoch 4 Lote 5400 Pérdida 0.8916 Precisión 0.4915\n",
      "Epoch 4 Lote 5450 Pérdida 0.8936 Precisión 0.4912\n",
      "Epoch 4 Lote 5500 Pérdida 0.8956 Precisión 0.4909\n",
      "Epoch 4 Lote 5550 Pérdida 0.8975 Precisión 0.4906\n",
      "Epoch 4 Lote 5600 Pérdida 0.8997 Precisión 0.4904\n",
      "Epoch 4 Lote 5650 Pérdida 0.9017 Precisión 0.4901\n",
      "Epoch 4 Lote 5700 Pérdida 0.9037 Precisión 0.4898\n",
      "Epoch 4 Lote 5750 Pérdida 0.9057 Precisión 0.4895\n",
      "Epoch 4 Lote 5800 Pérdida 0.9076 Precisión 0.4892\n",
      "Epoch 4 Lote 5850 Pérdida 0.9095 Precisión 0.4889\n",
      "Epoch 4 Lote 5900 Pérdida 0.9116 Precisión 0.4885\n",
      "Epoch 4 Lote 5950 Pérdida 0.9135 Precisión 0.4882\n",
      "Epoch 4 Lote 6000 Pérdida 0.9155 Precisión 0.4878\n",
      "Epoch 4 Lote 6050 Pérdida 0.9172 Precisión 0.4875\n",
      "Epoch 4 Lote 6100 Pérdida 0.9188 Precisión 0.4872\n",
      "Epoch 4 Lote 6150 Pérdida 0.9206 Precisión 0.4869\n",
      "Epoch 4 Lote 6200 Pérdida 0.9221 Precisión 0.4866\n",
      "Epoch 4 Lote 6250 Pérdida 0.9239 Precisión 0.4863\n",
      "Epoch 4 Lote 6300 Pérdida 0.9254 Precisión 0.4861\n",
      "Epoch 4 Lote 6350 Pérdida 0.9267 Precisión 0.4857\n",
      "Epoch 4 Lote 6400 Pérdida 0.9281 Precisión 0.4854\n",
      "Guardando checkpoint para el epoch 4 en ckpt/ckpt-9\n",
      "Tiempo que ha tardado 1 epoch: 3639.410322666168 segs\n",
      "\n",
      "Inicio del epoch 5\n",
      "Epoch 5 Lote 0 Pérdida 0.9189 Precisión 0.4564\n",
      "Epoch 5 Lote 50 Pérdida 1.0855 Precisión 0.4590\n",
      "Epoch 5 Lote 100 Pérdida 1.0999 Precisión 0.4565\n",
      "Epoch 5 Lote 150 Pérdida 1.1075 Precisión 0.4566\n",
      "Epoch 5 Lote 200 Pérdida 1.1040 Precisión 0.4567\n",
      "Epoch 5 Lote 250 Pérdida 1.1056 Precisión 0.4575\n",
      "Epoch 5 Lote 300 Pérdida 1.1047 Precisión 0.4577\n",
      "Epoch 5 Lote 350 Pérdida 1.0999 Precisión 0.4579\n",
      "Epoch 5 Lote 400 Pérdida 1.0962 Precisión 0.4580\n",
      "Epoch 5 Lote 450 Pérdida 1.0930 Precisión 0.4580\n",
      "Epoch 5 Lote 500 Pérdida 1.0890 Precisión 0.4579\n",
      "Epoch 5 Lote 550 Pérdida 1.0882 Precisión 0.4585\n",
      "Epoch 5 Lote 600 Pérdida 1.0882 Precisión 0.4583\n",
      "Epoch 5 Lote 650 Pérdida 1.0864 Precisión 0.4580\n",
      "Epoch 5 Lote 700 Pérdida 1.0845 Precisión 0.4587\n",
      "Epoch 5 Lote 750 Pérdida 1.0815 Precisión 0.4589\n",
      "Epoch 5 Lote 800 Pérdida 1.0773 Precisión 0.4595\n",
      "Epoch 5 Lote 850 Pérdida 1.0731 Precisión 0.4611\n",
      "Epoch 5 Lote 900 Pérdida 1.0684 Precisión 0.4623\n",
      "Epoch 5 Lote 950 Pérdida 1.0617 Precisión 0.4635\n",
      "Epoch 5 Lote 1000 Pérdida 1.0549 Precisión 0.4648\n",
      "Epoch 5 Lote 1050 Pérdida 1.0482 Precisión 0.4657\n",
      "Epoch 5 Lote 1100 Pérdida 1.0412 Precisión 0.4664\n",
      "Epoch 5 Lote 1150 Pérdida 1.0352 Precisión 0.4670\n",
      "Epoch 5 Lote 1200 Pérdida 1.0289 Precisión 0.4678\n",
      "Epoch 5 Lote 1250 Pérdida 1.0232 Precisión 0.4687\n",
      "Epoch 5 Lote 1300 Pérdida 1.0177 Precisión 0.4695\n",
      "Epoch 5 Lote 1350 Pérdida 1.0124 Precisión 0.4703\n",
      "Epoch 5 Lote 1400 Pérdida 1.0068 Precisión 0.4714\n",
      "Epoch 5 Lote 1450 Pérdida 1.0019 Precisión 0.4721\n",
      "Epoch 5 Lote 1500 Pérdida 0.9969 Precisión 0.4727\n",
      "Epoch 5 Lote 1550 Pérdida 0.9923 Precisión 0.4735\n",
      "Epoch 5 Lote 1600 Pérdida 0.9876 Precisión 0.4742\n",
      "Epoch 5 Lote 1650 Pérdida 0.9830 Precisión 0.4747\n",
      "Epoch 5 Lote 1700 Pérdida 0.9784 Precisión 0.4754\n",
      "Epoch 5 Lote 1750 Pérdida 0.9741 Precisión 0.4760\n",
      "Epoch 5 Lote 1800 Pérdida 0.9702 Precisión 0.4767\n",
      "Epoch 5 Lote 1850 Pérdida 0.9662 Precisión 0.4772\n",
      "Epoch 5 Lote 1900 Pérdida 0.9624 Precisión 0.4775\n",
      "Epoch 5 Lote 1950 Pérdida 0.9587 Precisión 0.4779\n",
      "Epoch 5 Lote 2000 Pérdida 0.9550 Precisión 0.4782\n",
      "Epoch 5 Lote 2050 Pérdida 0.9519 Precisión 0.4784\n",
      "Epoch 5 Lote 2100 Pérdida 0.9486 Precisión 0.4788\n",
      "Epoch 5 Lote 2150 Pérdida 0.9445 Precisión 0.4793\n",
      "Epoch 5 Lote 2200 Pérdida 0.9413 Precisión 0.4797\n",
      "Epoch 5 Lote 2250 Pérdida 0.9381 Precisión 0.4801\n",
      "Epoch 5 Lote 2300 Pérdida 0.9342 Precisión 0.4804\n",
      "Epoch 5 Lote 2350 Pérdida 0.9305 Precisión 0.4808\n",
      "Epoch 5 Lote 2400 Pérdida 0.9272 Precisión 0.4812\n",
      "Epoch 5 Lote 2450 Pérdida 0.9240 Precisión 0.4816\n",
      "Epoch 5 Lote 2500 Pérdida 0.9211 Precisión 0.4819\n",
      "Epoch 5 Lote 2550 Pérdida 0.9176 Precisión 0.4824\n",
      "Epoch 5 Lote 2600 Pérdida 0.9144 Precisión 0.4827\n",
      "Epoch 5 Lote 2650 Pérdida 0.9116 Precisión 0.4829\n",
      "Epoch 5 Lote 2700 Pérdida 0.9091 Precisión 0.4832\n",
      "Epoch 5 Lote 2750 Pérdida 0.9069 Precisión 0.4836\n",
      "Epoch 5 Lote 2800 Pérdida 0.9043 Precisión 0.4839\n",
      "Epoch 5 Lote 2850 Pérdida 0.9022 Precisión 0.4842\n",
      "Epoch 5 Lote 2900 Pérdida 0.9003 Precisión 0.4845\n",
      "Epoch 5 Lote 2950 Pérdida 0.8989 Precisión 0.4847\n",
      "Epoch 5 Lote 3000 Pérdida 0.8972 Precisión 0.4851\n",
      "Epoch 5 Lote 3050 Pérdida 0.8956 Precisión 0.4854\n",
      "Epoch 5 Lote 3100 Pérdida 0.8939 Precisión 0.4858\n",
      "Epoch 5 Lote 3150 Pérdida 0.8923 Precisión 0.4860\n",
      "Epoch 5 Lote 3200 Pérdida 0.8908 Precisión 0.4864\n",
      "Epoch 5 Lote 3250 Pérdida 0.8895 Precisión 0.4867\n",
      "Epoch 5 Lote 3300 Pérdida 0.8881 Precisión 0.4871\n",
      "Epoch 5 Lote 3350 Pérdida 0.8868 Precisión 0.4874\n",
      "Epoch 5 Lote 3400 Pérdida 0.8855 Precisión 0.4877\n",
      "Epoch 5 Lote 3450 Pérdida 0.8844 Precisión 0.4881\n",
      "Epoch 5 Lote 3500 Pérdida 0.8832 Precisión 0.4884\n",
      "Epoch 5 Lote 3550 Pérdida 0.8821 Precisión 0.4887\n",
      "Epoch 5 Lote 3600 Pérdida 0.8807 Precisión 0.4890\n",
      "Epoch 5 Lote 3650 Pérdida 0.8795 Precisión 0.4894\n",
      "Epoch 5 Lote 3700 Pérdida 0.8780 Precisión 0.4896\n",
      "Epoch 5 Lote 3750 Pérdida 0.8768 Precisión 0.4899\n",
      "Epoch 5 Lote 3800 Pérdida 0.8755 Precisión 0.4902\n",
      "Epoch 5 Lote 3850 Pérdida 0.8743 Precisión 0.4905\n",
      "Epoch 5 Lote 3900 Pérdida 0.8731 Precisión 0.4908\n",
      "Epoch 5 Lote 3950 Pérdida 0.8718 Precisión 0.4911\n",
      "Epoch 5 Lote 4000 Pérdida 0.8707 Precisión 0.4915\n",
      "Epoch 5 Lote 4050 Pérdida 0.8694 Precisión 0.4918\n",
      "Epoch 5 Lote 4100 Pérdida 0.8680 Precisión 0.4922\n",
      "Epoch 5 Lote 4150 Pérdida 0.8667 Precisión 0.4926\n",
      "Epoch 5 Lote 4200 Pérdida 0.8652 Precisión 0.4930\n",
      "Epoch 5 Lote 4250 Pérdida 0.8639 Precisión 0.4934\n",
      "Epoch 5 Lote 4300 Pérdida 0.8627 Precisión 0.4938\n",
      "Epoch 5 Lote 4350 Pérdida 0.8615 Precisión 0.4941\n",
      "Epoch 5 Lote 4400 Pérdida 0.8603 Precisión 0.4945\n",
      "Epoch 5 Lote 4450 Pérdida 0.8590 Precisión 0.4948\n",
      "Epoch 5 Lote 4500 Pérdida 0.8578 Precisión 0.4953\n",
      "Epoch 5 Lote 4550 Pérdida 0.8565 Precisión 0.4956\n",
      "Epoch 5 Lote 4600 Pérdida 0.8552 Precisión 0.4960\n",
      "Epoch 5 Lote 4650 Pérdida 0.8543 Precisión 0.4963\n",
      "Epoch 5 Lote 4700 Pérdida 0.8539 Precisión 0.4965\n",
      "Epoch 5 Lote 4750 Pérdida 0.8544 Precisión 0.4966\n",
      "Epoch 5 Lote 4800 Pérdida 0.8553 Precisión 0.4966\n",
      "Epoch 5 Lote 4850 Pérdida 0.8563 Precisión 0.4965\n",
      "Epoch 5 Lote 4900 Pérdida 0.8575 Precisión 0.4964\n",
      "Epoch 5 Lote 4950 Pérdida 0.8592 Precisión 0.4962\n",
      "Epoch 5 Lote 5000 Pérdida 0.8609 Precisión 0.4961\n",
      "Epoch 5 Lote 5050 Pérdida 0.8627 Precisión 0.4958\n",
      "Epoch 5 Lote 5100 Pérdida 0.8648 Precisión 0.4955\n",
      "Epoch 5 Lote 5150 Pérdida 0.8669 Precisión 0.4953\n",
      "Epoch 5 Lote 5200 Pérdida 0.8690 Precisión 0.4950\n",
      "Epoch 5 Lote 5250 Pérdida 0.8714 Precisión 0.4947\n",
      "Epoch 5 Lote 5300 Pérdida 0.8735 Precisión 0.4944\n",
      "Epoch 5 Lote 5350 Pérdida 0.8756 Precisión 0.4941\n",
      "Epoch 5 Lote 5400 Pérdida 0.8778 Precisión 0.4939\n",
      "Epoch 5 Lote 5450 Pérdida 0.8798 Precisión 0.4936\n",
      "Epoch 5 Lote 5500 Pérdida 0.8819 Precisión 0.4933\n",
      "Epoch 5 Lote 5550 Pérdida 0.8839 Precisión 0.4930\n",
      "Epoch 5 Lote 5600 Pérdida 0.8859 Precisión 0.4927\n",
      "Epoch 5 Lote 5650 Pérdida 0.8880 Precisión 0.4924\n",
      "Epoch 5 Lote 5700 Pérdida 0.8898 Precisión 0.4921\n",
      "Epoch 5 Lote 5750 Pérdida 0.8919 Precisión 0.4918\n",
      "Epoch 5 Lote 5800 Pérdida 0.8942 Precisión 0.4914\n",
      "Epoch 5 Lote 5850 Pérdida 0.8964 Precisión 0.4911\n",
      "Epoch 5 Lote 5900 Pérdida 0.8983 Precisión 0.4908\n",
      "Epoch 5 Lote 5950 Pérdida 0.9004 Precisión 0.4904\n",
      "Epoch 5 Lote 6000 Pérdida 0.9020 Precisión 0.4900\n",
      "Epoch 5 Lote 6050 Pérdida 0.9035 Precisión 0.4897\n",
      "Epoch 5 Lote 6100 Pérdida 0.9053 Precisión 0.4893\n",
      "Epoch 5 Lote 6150 Pérdida 0.9072 Precisión 0.4891\n",
      "Epoch 5 Lote 6200 Pérdida 0.9087 Precisión 0.4888\n",
      "Epoch 5 Lote 6250 Pérdida 0.9102 Precisión 0.4885\n",
      "Epoch 5 Lote 6300 Pérdida 0.9116 Precisión 0.4882\n",
      "Epoch 5 Lote 6350 Pérdida 0.9131 Precisión 0.4880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Lote 6400 Pérdida 0.9144 Precisión 0.4877\n",
      "Guardando checkpoint para el epoch 5 en ckpt/ckpt-10\n",
      "Tiempo que ha tardado 1 epoch: 3669.9833788871765 segs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "Transformer_train(model_Transformer,\n",
    "                  dataset,\n",
    "                  d_model=D_MODEL,\n",
    "                  train=TRAIN,\n",
    "                  epochs=EPOCHS,\n",
    "                  checkpoint_path=\"ckpt/\",\n",
    "                  max_to_keep=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nmzyRwDrRGdq"
   },
   "source": [
    "## Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cNHwJJrz3lPB"
   },
   "outputs": [],
   "source": [
    "def evaluate(inp_sentence):\n",
    "    inp_sentence = \\\n",
    "        [VOCAB_SIZE_EN-2] + processor_en.tokenizer.encode(inp_sentence) + [VOCAB_SIZE_EN-1]\n",
    "    enc_input = tf.expand_dims(inp_sentence, axis=0)\n",
    "    \n",
    "    output = tf.expand_dims([VOCAB_SIZE_ES-2], axis=0)\n",
    "    \n",
    "    for _ in range(MAX_LENGTH):\n",
    "        predictions = model_Transformer(enc_input, output, False) #(1, seq_length, VOCAB_SIZE_ES)\n",
    "        \n",
    "        prediction = predictions[:, -1:, :]\n",
    "        \n",
    "        predicted_id = tf.cast(tf.argmax(prediction, axis=-1), tf.int32)\n",
    "        \n",
    "        if predicted_id == VOCAB_SIZE_ES-1:\n",
    "            return tf.squeeze(output, axis=0)\n",
    "        \n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "        \n",
    "    return tf.squeeze(output, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s6VeFKrE6Kdx"
   },
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    output = evaluate(sentence).numpy()\n",
    "    \n",
    "    predicted_sentence = processor_es.tokenizer.decode(\n",
    "        [i for i in output if i < VOCAB_SIZE_ES-2]\n",
    "    )\n",
    "    \n",
    "    print(\"Entrada: {}\".format(sentence))\n",
    "    print(\"Traducción predicha: {}\".format(predicted_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrada: I have got a house\n",
      "Traducción predicha: Tengo una casa\n"
     ]
    }
   ],
   "source": [
    "translate(\"I have got a house\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "BupFjJlgDvCA",
    "outputId": "54e7aa51-1cc2-42a5-ede2-304d1a230d08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrada: This is a problem we have to solve.\n",
      "Traducción predicha: Es un problema que debemos solucionarlo.\n"
     ]
    }
   ],
   "source": [
    "translate(\"This is a problem we have to solve.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "ZdoWKbCP7Czs",
    "outputId": "dbe5425f-1c91-45a7-87ab-4ed0dbf13d74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrada: This is a really powerful tool!\n",
      "Traducción predicha: ¡Es una herramienta realmente poder!\n"
     ]
    }
   ],
   "source": [
    "translate(\"This is a really powerful tool!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "UGjBEb5WFMGt",
    "outputId": "a7892348-eaf4-4445-d816-3c587275f436"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrada: This is an interesting course about Natural Language Processing\n",
      "Traducción predicha: Es un punto interesante sobre el procedimiento de lenguas Lange\n"
     ]
    }
   ],
   "source": [
    "translate(\"This is an interesting course about Natural Language Processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L1v9g4T0FcO3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Transformer para NLP.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
